---
title: "Analyzes for Bachelor Theses"
author: "Julius Fenn"
format:
  html:
    toc: true
    toc-depth: 3
    html-math-method: katex
---


# Notes


# global variables

```{r}
## global variables: 
getBigTable = FALSE

```



# load merged pre-processed data

```{r}
#| echo: true
#| warning: false

# sets the directory of location of this script as the current directory
# setwd(dirname(rstudioapi::getSourceEditorContext()$path))

# load packages
require(pacman)
# regsem not needed!
p_load('tidyverse', 'jsonlite', 'magrittr', 'xlsx',
       'stargazer', 'psych', 'jtools', 'DT', 'ggstatsplot', 
       'lavaan', 'igraph',
       'regsem', 'MplusAutomation', 'MASS', 'report',
       'readxl')


# load data
setwd("outputs")
t12_questionnaireCAMs <- readRDS(file = "t12_questionnaireCAMs.rds")
CAMfiles <- readRDS(file = "CAMfiles.rds")
CAMdrawn <- readRDS(file = "CAMdrawn.rds")


t12_questionnaireCAMs$total_min_prolific[t12_questionnaireCAMs$total_min_prolific > 1000] <- NA


# load functions
setwd("../../functions")
for(i in 1:length(dir())){
  # print(dir()[i])
  source(dir()[i], encoding = "utf-8")
}


setwd("../functions_CAMapp")
for(i in 1:length(dir())){
  # print(dir()[i])
  source(dir()[i], encoding = "utf-8")
}
rm(i)
```


# Laras BA

## policy scale


```{r}
#| echo: true
#| warning: false


regEx <- "^policyItems"
nameScale <- "Policy Scale"
nameVariable <- "mean_PolicyItems"

### number of items
sum(str_detect(string = colnames(t12_questionnaireCAMs), pattern = regEx))

### only keep values with no missing values
tmp_dat <- t12_questionnaireCAMs[, c(str_subset(string = colnames(t12_questionnaireCAMs), pattern = regEx), "country.y")]

# tmp_dat <- tmp_dat[!is.na(tmp_dat$country.y), ]
tmp_dat <- tmp_dat[tmp_dat$country.y == "Germany" & !is.na(tmp_dat$country.y), ]
cat("analyes for country/ies:\n", unique(tmp_dat$country.y), "\n")

### get correlation plot, descriptives, EFA, CFA
tmp <- CFAstats(dataset = tmp_dat, regularExp = regEx, labelLatent = str_remove(string = nameVariable, pattern = "mean_"), 
                showPlots = TRUE, 
                computeEFA = TRUE, 
                computeCFA = TRUE, 
                computeCFAMplus = FALSE)

# ### variable mean
# questionnaireCAMs[[nameVariable]]  <- questionnaireCAMs %>%
#   select(matches(regEx)) %>%
#   rowMeans(na.rm = TRUE)
```

## stepwise LMs

prepare data

```{r}
subset_data_Lara <- t12_questionnaireCAMs[!is.na(t12_questionnaireCAMs$country.y), ]
subset_data_Lara <- t12_questionnaireCAMs[!is.na(t12_questionnaireCAMs$assortativity_valence_macro), ]
```

check correlation of network indicators:

```{r}
cor.plot(r = cor(subset_data_Lara[, str_subset(string = colnames(subset_data_Lara), pattern = "_macro")]))
```


remove single variables

```{r}
subset_data_Lara$mean_valence_normed_macro <- NULL
subset_data_Lara$meanDistance_directed_macro <- NULL
subset_data_Lara$diameter_unweighted_directed_macro <- NULL
subset_data_Lara$reciprocity_macro <- NULL
```

check again correlation of network indicators:

```{r}
cor.plot(r = cor(subset_data_Lara[, str_subset(string = colnames(subset_data_Lara), pattern = "_macro")]))
```


just copy & paste the formula:

```{r}
cat(paste0(str_subset(string = colnames(subset_data_Lara), pattern = "_macro"), collapse = " + "))
```



### for biospheric values

applying the variable "mean_BiosphericValues"

```{r}
#| results: asis


full.BiosphericValues <- lm(mean_BiosphericValues ~ mean_valence_macro + density_macro + transitivity_macro + centr_degree_macro + centr_clo_macro + centr_betw_macro + centr_eigen_macro + meanDistance_undirected_macro + diameter_weighted_undirected_macro + diameter_unweighted_undirected_macro + num_nodes_macro + num_nodes_pos_macro + num_nodes_neg_macro + num_nodes_neut_macro + num_nodes_ambi_macro + num_edges_macro + num_edges_solid_macro + num_edges_dashed_macro + num_edges_invaliddashed_macro + meanWeightEdges_macro + assortativity_valence_macro + assortativityDegree_macro, data = subset_data_Lara)
summary(full.BiosphericValues)


# Stepwise regression model
# help(stepwise)
step.BiosphericValues <- MASS::stepAIC(object = full.BiosphericValues, direction = "both",
                      trace = TRUE)
summary(step.BiosphericValues)

setwd("outputs/BA_Lara")
stargazer::stargazer(step.BiosphericValues, type = "html", out = "regressionBiosphericValues.html")

report::report(step.BiosphericValues)

out <- summary(step.BiosphericValues)
out$coefficients <- round(x = out$coefficients, digits = 2)
data.frame(DV = "mean_BiosphericValues", est = out$coefficients[ , 1], se = out$coefficients[ , 2], pvalue = out$coefficients[ , 4])
# ignore the first coefficient - only intercept
paste0(out$coefficients[ , 1], " (", out$coefficients[ , 2], ")")
```




# Amelies BA


## split into CAMs from Germany

```{r}
setwd("outputs/BA_Amelie")
if(!file.exists("CAMs_Germany")){
  dir.create("CAMs_Germany")
}
setwd("CAMs_Germany")


CAMfiles_Germany <- CAMfiles

## check is ID data set is complete
if(!all(CAMfiles_Germany[[1]]$participantCAM %in% t12_questionnaireCAMs$PROLIFIC_PID)){
    print("Error")
}else{
  tmp_ids <- t12_questionnaireCAMs$PROLIFIC_PID[t12_questionnaireCAMs$country.x == "Germany"]
  
  ## keep only CAM data from Germany
  CAMfiles_Germany[[1]] <- CAMfiles_Germany[[1]][CAMfiles_Germany[[1]]$participantCAM %in% tmp_ids,]
  CAMfiles_Germany[[2]] <- CAMfiles_Germany[[2]][CAMfiles_Germany[[2]]$participantCAM %in% tmp_ids,]
  CAMfiles_Germany[[3]] <- CAMfiles_Germany[[3]][CAMfiles_Germany[[3]]$participantCAM.x %in% tmp_ids,]
  
  ## save files ob subsets
  vroom::vroom_write(x =  CAMfiles_Germany[[1]], file = "CAM_nodes_Germany.txt")
  vroom::vroom_write(x =  CAMfiles_Germany[[2]], file = "CAM_connectors_Germany.txt")
  vroom::vroom_write(x =  CAMfiles_Germany[[3]], file = "CAM_merged_Germany.txt")
}
```

### aggregate CAMs

```{r}
sel_ids <- unique(CAMfiles_Germany[[1]]$participantCAM)

tmp_nodes <- CAMfiles_Germany[[1]]

tmp_nodes$text_summarized <- str_remove(string = tmp_nodes$text_summarized, pattern = "_positive$|_negative$|_neutral$|_ambivalent$")
tmp_nodes$text_summarized <- str_trim(string = tmp_nodes$text_summarized)


CAMaggregated_Germany <- aggregate_CAMs(dat_merged = CAMfiles_Germany[[3]], dat_nodes = tmp_nodes,
                                ids_CAMs = sel_ids)

g = CAMaggregated_Germany[[2]]
g2 = simplify(CAMaggregated_Germany[[2]])
# plot(g2, edge.arrow.size=0.01,
#      vertex.size=diag(CAMaggregated_Germany[[1]]) / max(diag(CAMaggregated_Germany[[1]]))*20)

E(g2)$weight = sapply(E(g2), function(e) {
  length(all_shortest_paths(g, from=ends(g2, e)[1], to=ends(g2, e)[2])$res) } )
E(g2)$weight = E(g2)$weight / 2
# E(g2)$weight[E(g2)$weight == 1] <- NA

V(g2)$color[V(g2)$value <= .5 & V(g2)$value >= -.5] <- "yellow"

V(g2)$shape <- NA
V(g2)$shape <- ifelse(test = V(g2)$color == "yellow", yes = "square", no = "circle")


sort(diag(CAMaggregated_Germany[[1]]) / length(sel_ids) * 100)


### > plot multiple times because of random layout
for(i in 1:5){
plot(g2, edge.arrow.size = 0,
     layout=layout_nicely, vertex.frame.color="black", asp = .5, margin = -0.1,
     vertex.size=diag(CAMaggregated_Germany[[1]]) / max(diag(CAMaggregated_Germany[[1]]))*7,
     vertex.label.cex = .9, 
     edge.weight=2, edge.width=(E(g2)$weight/5))
}
```




## split into CAMs from USA


```{r}
setwd("outputs/BA_Amelie")
if(!file.exists("CAMs_USA")){
  dir.create("CAMs_USA")
}
setwd("CAMs_USA")


CAMfiles_USA <- CAMfiles

## check is ID data set is complete
if(!all(CAMfiles_USA[[1]]$participantCAM %in% t12_questionnaireCAMs$PROLIFIC_PID)){
    print("Error")
}else{
  tmp_ids <- t12_questionnaireCAMs$PROLIFIC_PID[t12_questionnaireCAMs$country.x == "USA"]
  
  ## keep only CAM data from Germany
  CAMfiles_USA[[1]] <- CAMfiles_USA[[1]][CAMfiles_USA[[1]]$participantCAM %in% tmp_ids,]
  CAMfiles_USA[[2]] <- CAMfiles_USA[[2]][CAMfiles_USA[[2]]$participantCAM %in% tmp_ids,]
  CAMfiles_USA[[3]] <- CAMfiles_USA[[3]][CAMfiles_USA[[3]]$participantCAM.x %in% tmp_ids,]
  
  ## save files ob subsets
  vroom::vroom_write(x =  CAMfiles_USA[[1]], file = "CAM_nodes_USA.txt")
  vroom::vroom_write(x =  CAMfiles_USA[[2]], file = "CAM_connectors_USA.txt")
  vroom::vroom_write(x =  CAMfiles_USA[[3]], file = "CAM_merged_USA.txt")
}
```

### aggregate CAMs

```{r}
sel_ids <- unique(CAMfiles_USA[[1]]$participantCAM)

tmp_nodes <- CAMfiles_USA[[1]]

tmp_nodes$text_summarized <- str_remove(string = tmp_nodes$text_summarized, pattern = "_positive$|_negative$|_neutral$|_ambivalent$")
tmp_nodes$text_summarized <- str_trim(string = tmp_nodes$text_summarized)


CAMaggregated_USA <- aggregate_CAMs(dat_merged = CAMfiles_USA[[3]], dat_nodes = tmp_nodes,
                                ids_CAMs = sel_ids)

g = CAMaggregated_USA[[2]]
g2 = simplify(CAMaggregated_USA[[2]])
# plot(g2, edge.arrow.size=0.01,
#      vertex.size=diag(CAMaggregated_USA[[1]]) / max(diag(CAMaggregated_USA[[1]]))*20)

E(g2)$weight = sapply(E(g2), function(e) {
  length(all_shortest_paths(g, from=ends(g2, e)[1], to=ends(g2, e)[2])$res) } )
E(g2)$weight = E(g2)$weight / 2
# E(g2)$weight[E(g2)$weight == 1] <- NA

V(g2)$color[V(g2)$value <= .5 & V(g2)$value >= -.5] <- "yellow"

V(g2)$shape <- NA
V(g2)$shape <- ifelse(test = V(g2)$color == "yellow", yes = "square", no = "circle")



sort(diag(CAMaggregated_USA[[1]]) / length(sel_ids) * 100)


### > plot multiple times because of random layout
for(i in 1:5){
plot(g2, edge.arrow.size = 0,
     layout=layout_nicely, vertex.frame.color="black", asp = .5, margin = -0.1,
     vertex.size=diag(CAMaggregated_USA[[1]]) / max(diag(CAMaggregated_USA[[1]]))*7,
     vertex.label.cex = .9, 
     edge.weight=2, edge.width=(E(g2)$weight/5))
}
```


# Magdalenas BA

## aggregate CAMs

```{r}
sel_ids <- unique(CAMfiles[[1]]$participantCAM)

tmp_nodes <- CAMfiles[[1]]

tmp_nodes$text_summarized <- str_remove(string = tmp_nodes$text_summarized, pattern = "_positive$|_negative$|_neutral$|_ambivalent$")
tmp_nodes$text_summarized <- str_trim(string = tmp_nodes$text_summarized)


CAMaggregated <- aggregate_CAMs(dat_merged = CAMfiles[[3]], dat_nodes = tmp_nodes,
                                ids_CAMs = sel_ids)

g = CAMaggregated[[2]]
g2 = simplify(CAMaggregated[[2]])
# plot(g2, edge.arrow.size=0.01,
#      vertex.size=diag(CAMaggregated[[1]]) / max(diag(CAMaggregated[[1]]))*20)

E(g2)$weight = sapply(E(g2), function(e) {
  length(all_shortest_paths(g, from=ends(g2, e)[1], to=ends(g2, e)[2])$res) } )
E(g2)$weight = E(g2)$weight / 2
# E(g2)$weight[E(g2)$weight == 1] <- NA

V(g2)$color[V(g2)$value <= .5 & V(g2)$value >= -.5] <- "yellow"

V(g2)$shape <- NA
V(g2)$shape <- ifelse(test = V(g2)$color == "yellow", yes = "square", no = "circle")



sort(diag(CAMaggregated[[1]]) / length(sel_ids) * 100)


### > plot multiple times because of random layout
for(i in 1:5){
plot(g2, edge.arrow.size = 0,
     layout=layout_nicely, vertex.frame.color="black", asp = .5, margin = -0.1,
     vertex.size=diag(CAMaggregated[[1]]) / max(diag(CAMaggregated[[1]]))*7,
     vertex.label.cex = .9, 
     edge.weight=2, edge.width=(E(g2)$weight/5))
}
```


## get the first 8 concepts drawn in CAMs


```{r}
#| echo: true
#| warning: false


### only keep t2
tmp_dat <- t12_questionnaireCAMs[!is.na(t12_questionnaireCAMs$country.y), ]


affectiveImagery_CAMs <- tmp_dat[, c("PROLIFIC_PID", "country.y", "classes_conspiracy", "politicalParty", "socio_sex",
  "mean_affImg", "mean_valence_macro", "num_nodes_macro",
  str_subset(string = colnames(tmp_dat), pattern = "^R[:digit:]"), 
  sort(str_subset(string = colnames(tmp_dat), pattern = "^affImg")))]
sum(is.na(affectiveImagery_CAMs$mean_affImg))

affectiveImagery_CAMs[, paste0("A", 1:8)] <- NA

for(i in 1:nrow(tmp_dat)){
  tmp_concepts <- CAMfiles[[1]][CAMfiles[[1]]$participantCAM == tmp_dat$PROLIFIC_PID[i],]
  tmp_concepts <- tmp_concepts$text[order(tmp_concepts$date, decreasing = FALSE)][2:9]
  
  affectiveImagery_CAMs[i, str_subset(string = colnames(affectiveImagery_CAMs), pattern = "^A")] <- tmp_concepts
}

### save data set
setwd("outputs/BA_Magdalena")
if(!file.exists("data_AffImg_CAMs")){
  dir.create("data_AffImg_CAMs")
}
setwd("data_AffImg_CAMs")

## save as .xlsx file
xlsx::write.xlsx2(x = affectiveImagery_CAMs, file = "affectiveImagery_CAMs.xlsx")
## save as .csv file
write.csv2(x = affectiveImagery_CAMs, file = "affectiveImagery_CAMs.csv")
## save as R object
saveRDS(affectiveImagery_CAMs, file = "affectiveImagery_CAMs.rds")
```



## clean affective imagery


create relevant subset:

```{r}
#| echo: true
#| warning: false

affectiveImagery <- t12_questionnaireCAMs[, c("PROLIFIC_PID", "country.x", "classes_conspiracy", "politicalParty", "socio_sex",
  "mean_affImg",
  str_subset(string = colnames(tmp_dat), pattern = "^R[:digit:]"), 
  sort(str_subset(string = colnames(tmp_dat), pattern = "^affImg")))]
sum(is.na(affectiveImagery$mean_affImg))
```


use approximate matching only for German (not run automatically!):

```{r}
#| eval: false


subset_affectiveImagery <- affectiveImagery[affectiveImagery$country.x == "Germany",]

tmp_vec <- unlist(subset_affectiveImagery[, str_subset(string = colnames(tmp_dat), pattern = "^R[:digit:]")])
tmp_vec <- tmp_vec[!is.na(tmp_vec)]

applyApproximate <- data.frame(originalWord = tmp_vec, summarizedWord = tmp_vec)


rm(subset_affectiveImagery); rm(tmp_vec)

length(applyApproximate$originalWord)
length(unique(applyApproximate$originalWord))
names(table(applyApproximate$originalWord))[table(applyApproximate$originalWord) >= 10]


readkey <- function()
{
  # cat ('\nPress [enter] to continue\n')
  # cat('or write "aaa" in the console to terminate loop\n')

  line <- readline()
  if(line == "a123"){
    cat("for loop terminated, please write down the last round where you have summarized words:",
        i, "\n")
    stop("process terminated")
  }else{
    return(line)
  }
}

defineDistance = 2

for(i in 1:nrow(applyApproximate)){
  print(i)
  
  dist <- stringdist::stringdist(a = applyApproximate$summarizedWord[i], applyApproximate$summarizedWord)


  
if(!all(unique(applyApproximate$summarizedWord[dist <= defineDistance]) == applyApproximate$summarizedWord[i])){
  
  cat("\n if you want to summarize the followings words:\n")
  print(unique(applyApproximate$summarizedWord[dist <= defineDistance]))
  
  cat('>>> provide a superordinate word, else write "c" (for continue) or write "a123" to stop the summary process')

 
  out <- readkey() 
  # print(out)
  
  if(out != "c"){
      applyApproximate$summarizedWord[applyApproximate$summarizedWord %in% unique(applyApproximate$summarizedWord[dist <= defineDistance])] <- out
  }
}
}


### save data set
setwd("outputs/BA_Magdalena")
if(!file.exists("data")){
  dir.create("data_AffImg")
}
setwd("data_AffImg")

## save as .xlsx file
xlsx::write.xlsx2(x = applyApproximate, file = "applyApproximate.xlsx")
## save as .csv file
write.csv2(x = applyApproximate, file = "applyApproximate.csv")
## save as R object
saveRDS(applyApproximate, file = "applyApproximate.rds")
```

## overwrite affective imagery

### create long format data

```{r}
### create long format data
counter = 1
for(p in affectiveImagery$PROLIFIC_PID){
  tmp_dat <- affectiveImagery[affectiveImagery$PROLIFIC_PID == p, ]
  
  tmp_vec_word <- unlist(tmp_dat[, str_subset(string = colnames(tmp_dat), pattern = "^R[:digit:]")])
  tmp_vec_word <- tmp_vec_word[!is.na(tmp_vec_word)]
  
  
    tmp_vec_valence <- unlist(tmp_dat[, str_subset(string = colnames(tmp_dat), pattern = "^affImgAffect")])
  tmp_vec_valence <- tmp_vec_valence[!is.na(tmp_vec_valence)]
  
  if(length(tmp_vec_word) != length(tmp_vec_valence)){
    tmp_vec_valence <- rep(NA, times = length(tmp_vec_word))
  }
  
  if(length(tmp_vec_word) == 0 & length(tmp_vec_valence) == 0){
    tmp_vec_word <- NA
    tmp_vec_valence <- NA
  }
  
  if(counter == 1){
    dat_out <- data.frame(PROLIFIC_PID = tmp_dat$PROLIFIC_PID, 
             country = tmp_dat$country.x,
             class_conspiracy = tmp_dat$classes_conspiracy, 
             originalWord = tmp_vec_word,
             valence = tmp_vec_valence)
  }else{
    dat_out <- rbind(dat_out, data.frame(PROLIFIC_PID = tmp_dat$PROLIFIC_PID, 
             country = tmp_dat$country.x,
             class_conspiracy = tmp_dat$classes_conspiracy, 
             originalWord = tmp_vec_word,
             valence = tmp_vec_valence))
  }
  
  counter = counter + 1
}
```


### overwrite approximate matching

Zwischenschritt vor Ãœbersetzung - grobe Zusammenfassung

```{r}
setwd("data_BA_Magdalena")

overwriteApproximate <- read.xlsx2(file = "AffectiveImageryDataGermanSummarised.xlsx", sheetIndex = 1)


dat_out$summarizedWord <- dat_out$originalWord
# w <- "Real"

length(unique(dat_out$summarizedWord))

for(w in unique(overwriteApproximate$originalWord)){
  dat_out$summarizedWord[dat_out$originalWord %in% w & dat_out$country == "Germany"] <- overwriteApproximate$summarizedWord_final[overwriteApproximate$originalWord %in% w]
}

length(unique(dat_out$summarizedWord))


setwd("..")

### save data set
setwd("outputs/BA_Magdalena")
if(!file.exists("data_AffImg_processed")){
  dir.create("data_AffImg_processed")
}
setwd("data_AffImg_processed")

## save as .xlsx file
xlsx::write.xlsx2(x = dat_out, file = "affImg.xlsx")
## save as .csv file
write.csv2(x = dat_out, file = "affImg.csv")
## save as R object
saveRDS(dat_out, file = "affImg.rds")
```

### overwrite translated words

```{r}
setwd("data_BA_Magdalena")

overwriteApproximate <- read.xlsx2(file = "AffectiveImageryDataGermanTranslated.xlsx", sheetIndex = 1)

overwriteApproximate <- overwriteApproximate[overwriteApproximate$country == "Germany", ]


# dat_out$summarizedWord <- dat_out$originalWord
# w <-  unique(overwriteApproximate$originalWord)[2]

length(unique(dat_out$summarizedWord))

for(w in unique(overwriteApproximate$originalWord)){
  dat_out$summarizedWord[dat_out$originalWord %in% w & dat_out$country == "Germany"] <- overwriteApproximate$summarizedWord_final[overwriteApproximate$originalWord %in% w]
}

length(unique(dat_out$summarizedWord))


setwd("..")

# ### save data set
setwd("outputs/BA_Magdalena")
if(!file.exists("data_AffImg_processed")){
  dir.create("data_AffImg_processed")
}
setwd("data_AffImg_processed")

## save as .xlsx file
xlsx::write.xlsx2(x = dat_out, file = "affImg_translated.xlsx")
## save as .csv file
write.csv2(x = dat_out, file = "affImg_translated.csv")
## save as R object
saveRDS(dat_out, file = "affImg_translated.rds")
```

### statistics

```{r}
dat_out %>%
  group_by(class_conspiracy) %>%
  summarise(N = n(), mean = mean(valence, na.rm = TRUE))


dat_out %>%
  group_by(class_conspiracy, country) %>%
  summarise(N = n(), mean = mean(valence, na.rm = TRUE))
```


## clean affective imagery after translation

NOT DONE

use approximate matching for English and German (not run automatically!):

```{r}
#| eval: false


## remove NAs
dat_out$summarizedWord[is.na(dat_out$summarizedWord)] <- ""

readkey <- function()
{
  # cat ('\nPress [enter] to continue\n')
  # cat('or write "aaa" in the console to terminate loop\n')

  line <- readline()
  if(line == "a123"){
    cat("for loop terminated, please write down the last round where you have summarized words:",
        i, "\n")
    stop("process terminated")
  }else{
    return(line)
  }
}

defineDistance = 1




for(i in 1:nrow(dat_out)){
  print(i)
  
  dist <- stringdist::stringdist(a = dat_out$summarizedWord[i], dat_out$summarizedWord)


  
if(!all(unique(dat_out$summarizedWord[dist <= defineDistance]) == dat_out$summarizedWord[i])){
  
  cat("\n if you want to summarize the followings words:\n")
  print(unique(dat_out$summarizedWord[dist <= defineDistance]))
  
  cat('>>> provide a superordinate word, else write "c" (for continue) or write "a123" to stop the summary process')

 
  out <- readkey() 
  # print(out)
  
  if(out != "c"){
      dat_out$summarizedWord[dat_out$summarizedWord %in% unique(dat_out$summarizedWord[dist <= defineDistance])] <- out
  }
}
}




### save data set
setwd("outputs/BA_Magdalena")
if(!file.exists("data_AffImg")){
  dir.create("data_AffImg")
}
setwd("data_AffImg")

## save as .xlsx file
xlsx::write.xlsx2(x = applyApproximate, file = "applyApproximate_translated.xlsx")
## save as .csv file
write.csv2(x = applyApproximate, file = "applyApproximate_translated.csv")
## save as R object
saveRDS(applyApproximate, file = "applyApproximate_translated.rds")
```



## get tables for ratings

```{r}
if(getBigTable){
  ### save data set
setwd("outputs/BA_Magdalena")
if(!file.exists("savedWordlists_combined")){
  dir.create("savedWordlists_combined")
}
setwd("savedWordlists_combined")

### create tables for ratings
for(i in 1:length(unique(CAMfiles[[1]]$participantCAM))){
  
  ## t1 - Affective Imagery
  tmp_associations <- dat_out[dat_out$PROLIFIC_PID %in% unique(CAMfiles[[1]]$participantCAM)[i], ]

  ## t2 - CAM
  tmp_concepts <- CAMfiles[[1]][CAMfiles[[1]]$participantCAM %in% unique(CAMfiles[[1]]$participantCAM)[i], ]

  ## add variables Affective Imagery
  tmp_associations$type <- "Affective Imagery"
  tmp_associations$Rating_Magdalena <- NA
  tmp_associations$Comment_Magdalena <- NA
  
  tmp_associations$id <- NA
  tmp_associations$comment <- NA
  
  tmp_associations <- tmp_associations[, c("type", "PROLIFIC_PID", "id", "summarizedWord", "valence", "comment", "Rating_Magdalena", "Comment_Magdalena")]

  colnames(tmp_associations)[4] <- "text"
  
  ## add variables CAM
  tmp_concepts$type <- "CAM"
  tmp_concepts$Rating_Magdalena <- NA
  tmp_concepts$Comment_Magdalena <- NA
  
  ## order chronologically
  tmp_concepts <- tmp_concepts[order(tmp_concepts$dateConceptCreated), ]
  tmp_concepts <- tmp_concepts[1:9, ] # first 8 concepts

  
  tmp_concepts <- tmp_concepts[, c("type", "participantCAM", "id", "text", "value", "comment", "Rating_Magdalena", "Comment_Magdalena")]
  
  colnames(tmp_concepts)[2] <- "PROLIFIC_PID"
  colnames(tmp_concepts)[5] <- "valence"
  
  
  tmp_dat <- rbind(tmp_associations, tmp_concepts)

  if(i == 1){
    write.xlsx2(x = tmp_dat, file="ratingsAffimgCAMs.xlsx", sheetName=unique(tmp_concepts$PROLIFIC_PID), row.names=FALSE)
  }else{
    write.xlsx2(tmp_dat, file="ratingsAffimgCAMs.xlsx", sheetName=unique(tmp_concepts$PROLIFIC_PID), row.names=FALSE, append=TRUE)
  }
  
      write.xlsx2(x = tmp_dat, file=paste0(unique(tmp_concepts$PROLIFIC_PID), ".xlsx"), sheetName=unique(tmp_concepts$PROLIFIC_PID), row.names=FALSE)

}
}
```
```{r}
### save data set
setwd("outputs/BA_Magdalena")
setwd("savedWordlists_combined")

# file <- system.file("tests", "ratingsAffimgCAMs.xlsx", package = "xlsx")

wb <- loadWorkbook("ratingsAffimgCAMs.xlsx")
sheets <- getSheets(wb)
length(sheets)
```




## read tables for ratings


```{r}
setwd("data_BA_Magdalena")

# Path to your Excel file
file_path <- "ratingsAffimgCAMs_rated.xlsx"

# List all sheet names
sheet_names <- excel_sheets(file_path)

# Read all sheets into a list of data frames
list_of_data_frames <- lapply(sheet_names, function(sheet) read_excel(file_path, sheet = sheet))
length(list_of_data_frames)


 
for(i in 1:length(list_of_data_frames)){
  if(i == 1){
    combined_ratings <- list_of_data_frames[[i]]
  }else{
    combined_ratings <- rbind(combined_ratings, list_of_data_frames[[i]])
  }
}


sum(is.na(combined_ratings$Rating_Magdalena))
unique(combined_ratings$PROLIFIC_PID[is.na(combined_ratings$Rating_Magdalena)])

vec_overalp <- c()
for(i in 1:length(unique(combined_ratings$PROLIFIC_PID))){
  tmp_ratings <- combined_ratings[combined_ratings$PROLIFIC_PID == unique(combined_ratings$PROLIFIC_PID)[i], ]

 vec_overalp[i] <- sum(tmp_ratings$Rating_Magdalena[tmp_ratings$type == "Affective Imagery"]) / sum(tmp_ratings$type == "Affective Imagery") * 100
}

summary(vec_overalp)
```

## read affective imagery with coding


```{r}
setwd("data_BA_Magdalena")

AIT_coded <- xlsx::read.xlsx2(file = "AffectiveImagery_coded.xlsx", sheetIndex = 1)
AIT_coded[AIT_coded$PROLIFIC_PID == "",]
AIT_coded <- AIT_coded[AIT_coded$PROLIFIC_PID != "",]

## remove suffix
CAMs_all <- CAMfiles[[1]]

CAMs_all$text_summarized <- str_remove(string = CAMs_all$text_summarized, pattern = "_positive$|_negative$|_neutral$|_ambivalent$")
CAMs_all$text_summarized <- str_trim(string = CAMs_all$text_summarized)


for(i in 1:length(unique(CAMs_all$participantCAM))){
  tmp_AIT <- AIT_coded[AIT_coded$PROLIFIC_PID == unique(CAMs_all$participantCAM)[i], ]
  tmp_CAM <- CAMs_all[CAMs_all$participantCAM == unique(CAMs_all$participantCAM)[i], ]
  
  table(tmp_AIT$codedWords)
  table(tmp_CAM$text_summarized)
}
```
## fix AIT

```{r}
setwd("data_BA_Magdalena")

AIT_coded_fixed <- xlsx::read.xlsx2(file = "AffectiveImagery_coded_fixed.xlsx", sheetIndex = 1)
# AIT_coded[AIT_coded$PROLIFIC_PID == "",]
# AIT_coded <- AIT_coded[AIT_coded$PROLIFIC_PID != "",]

AIT_coded_fixed$fixedWords <- str_trim(string = AIT_coded_fixed$fixedWords, side = "both")
sort(table(AIT_coded_fixed$fixedWords))
```




## create wordlists

### for CAMs

```{r}
CAMwordlist <- create_wordlist(
  dat_nodes =  CAMfiles[[1]],
  dat_merged =  CAMfiles[[3]],
  order = "frequency",
  splitByValence = FALSE,
  comments = TRUE,
  raterSubsetWords = NULL,
  rater = FALSE
)


DT::datatable(data = CAMwordlist)

setwd("outputs/BA_Magdalena/aggregated")
write.xlsx2(x = CAMwordlist, file = "CAMwordlist.xlsx")
```


### for AIT



```{r}
AIT_coded_fixed$valence <- as.numeric(AIT_coded_fixed$valence)
AIT_coded_fixed$valence[is.na(AIT_coded_fixed$valence)] <- 4 # !!!

AIT_coded_fixed$fixedWords <- str_trim(string = AIT_coded_fixed$fixedWords, side = "both")

freq_terms = AIT_coded_fixed %>% dplyr::select(fixedWords) %>% dplyr::count(fixedWords)
colnames(freq_terms) <- c("Words", "all")
freq_terms <- as.data.frame(freq_terms)

freq_terms <- freq_terms[order(freq_terms$all, decreasing = TRUE),]

freq_terms$mean_valence <- NA
freq_terms$sd_valence <- NA


for(i in 1:nrow(freq_terms)){
  freq_terms$mean_valence[i] <- mean(AIT_coded_fixed$valence[AIT_coded_fixed$fixedWords == freq_terms$Words[i]])
  freq_terms$sd_valence[i] <- sd(AIT_coded_fixed$valence[AIT_coded_fixed$fixedWords == freq_terms$Words[i]])
  
  if(is.na( freq_terms$mean_valence[i])){
    print(i)
  }
}


DT::datatable(data = freq_terms)


setwd("outputs/BA_Magdalena/aggregated")
write.xlsx2(x = freq_terms, file = "AITwordlist.xlsx")
```


## aggregate data


### aggregate AIT

get function:

```{r}
# word_sequences = sample_data
# window_length = 2
# Define the function to create a semantic network
create_semantic_network <- function(word_sequences, window_length = 2, verbose = FALSE) {
  # Initialize an empty graph
  g <- make_empty_graph(n = 0, directed = FALSE)
  
  # Loop through each sequence of words
  for (sequence in word_sequences) {
    if(verbose){
      cat("\n\n current sequence of words:\n", sequence, "\n, whereby the central word currently is:\n")
    }

    sequence_length <- length(sequence)
    
    # Loop through each word in the sequence
    for (i in 1:sequence_length) {
      # Determine the bounds of the window
      window_start <- max(1, i - window_length)
      window_end <- min(sequence_length, i + window_length)
      
      # Get the central word
      central_word <- sequence[i]
      
      if(verbose){
        cat("\n", central_word, "\n, with the neighboring words:\n")
      }
      # Loop through each word in the window
      for (j in window_start:window_end) {
        if (i != j) { # Avoid self-loops
          # Get the neighboring word
          neighbor_word <- sequence[j]
          
          if(verbose){
            cat(neighbor_word, "\n")
          }

          
          # Add both words to the graph if not already present
          if (!central_word %in% V(g)$name) {
            g <- add_vertices(g, 1, name = central_word)
          }
          if (!neighbor_word %in% V(g)$name) {
            g <- add_vertices(g, 1, name = neighbor_word)
          }
          
          # Add an edge between the central word and the neighboring word
          g <- add_edges(g, sort(c(central_word, neighbor_word)))
         }
      }
    }
  }
  
  # Simplify the graph to remove multiple edges and loops
  # g <- simplify(g)
  
  return(g)
}


```

apply function:

```{r}
AIT_coded_fixed_subset <- AIT_coded_fixed[AIT_coded_fixed$PROLIFIC_PID %in% unique(CAMfiles[[1]]$participantCAM),]

list_AIT <- list()
for(p in unique(AIT_coded_fixed_subset$PROLIFIC_PID)){
  tmp_vec <- AIT_coded_fixed_subset[AIT_coded_fixed_subset$PROLIFIC_PID == p, c("fixedWords")]
  list_AIT[[p]] <- tmp_vec
}


# Create the network
network <- create_semantic_network(word_sequences = list_AIT, window_length = 2)

# Plot the network
plot(network)

degree(graph = network)
adjacency_matrix <- get.adjacency(network)

network_weighted <- graph.adjacency(get.adjacency(network),weighted=TRUE)
network_weighted <- simplify(graph = network_weighted)
# edge.attributes(graph = network_weighted)


rownames(adjacency_matrix)[!rownames(adjacency_matrix) %in% freq_terms$Words]

freq_terms_sorted <- freq_terms[freq_terms$Words %in%  rownames(adjacency_matrix), ]
freq_terms_sorted <- freq_terms_sorted[order(match(freq_terms_sorted$Words, rownames(adjacency_matrix))), ]

V(network_weighted)$color <- "yellow"


V(network_weighted)$color[freq_terms_sorted$mean_valence <= 3.5] <- "red"
V(network_weighted)$color[freq_terms_sorted$mean_valence >= 4.5] <- "green"

for(i in 1:5){
plot(network_weighted, edge.arrow.size = 0,
     layout=layout_nicely, vertex.frame.color="black", asp = .5, margin = -0.1,
     vertex.size=freq_terms_sorted$all / max(freq_terms_sorted$all)*7,
     vertex.label.cex = .9, 
     edge.weight=2, edge.width=(E(network_weighted)$weight/5))
}
```


### aggregate CAMs

```{r}
sel_ids <- unique(CAMfiles[[1]]$participantCAM)

tmp_nodes <- CAMfiles[[1]]

tmp_nodes$text_summarized <- str_remove(string = tmp_nodes$text_summarized, pattern = "_positive$|_negative$|_neutral$|_ambivalent$")
tmp_nodes$text_summarized <- str_trim(string = tmp_nodes$text_summarized)


CAMaggregated_Germany <- aggregate_CAMs(dat_merged = CAMfiles[[3]], dat_nodes = tmp_nodes,
                                ids_CAMs = sel_ids)

g = CAMaggregated_Germany[[2]]
g2 = simplify(CAMaggregated_Germany[[2]])
# plot(g2, edge.arrow.size=0.01,
#      vertex.size=diag(CAMaggregated_Germany[[1]]) / max(diag(CAMaggregated_Germany[[1]]))*20)

E(g2)$weight = sapply(E(g2), function(e) {
  length(all_shortest_paths(g, from=ends(g2, e)[1], to=ends(g2, e)[2])$res) } )
E(g2)$weight = E(g2)$weight / 2
# E(g2)$weight[E(g2)$weight == 1] <- NA

V(g2)$color[V(g2)$value <= .5 & V(g2)$value >= -.5] <- "yellow"

V(g2)$shape <- NA
V(g2)$shape <- ifelse(test = V(g2)$color == "yellow", yes = "square", no = "circle")


sort(diag(CAMaggregated_Germany[[1]]) / length(sel_ids) * 100)


### > plot multiple times because of random layout
for(i in 1:5){
plot(g2, edge.arrow.size = 0,
     layout=layout_nicely, vertex.frame.color="black", asp = .5, margin = -0.1,
     vertex.size=diag(CAMaggregated_Germany[[1]]) / max(diag(CAMaggregated_Germany[[1]]))*7,
     vertex.label.cex = .9, 
     edge.weight=2, edge.width=(E(g2)$weight/5))
}
```
