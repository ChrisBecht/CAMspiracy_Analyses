---
title: "Analyze Merged Files from t1, t2"
author: "Julius Fenn, Christophe Becht"
format:
  html:
    toc: true
    toc-depth: 3
    html-math-method: katex
---


# Notes

```{r}
## global variables:
consider_Protocol <- TRUE
```



# load merged pre-processed data

```{r}
#| echo: true
#| warning: false

# sets the directory of location of this script as the current directory
setwd(dirname(rstudioapi::getSourceEditorContext()$path))

### load R packages
require(pacman)

p_load('tidyverse', 'ggplot2', 'RColorBrewer',
       'car', 'lsr',
       'jsonlite', 'magrittr', 
       'xlsx', 'openxlsx',
       'stargazer', 'psych', 'jtools', 'DT', 'ggstatsplot', 
       'lavaan', 'igraph',
       'reticulate')


virtualenv_create("r-reticulate")
# library(leiden)
#'regsem', 'MplusAutomation'

### load Python modules
# reticulate::py_config()
# reticulate::py_module_available("pycairo")

if(!reticulate::py_module_available("igraph")){
  py_install("igraph") # pip install python-igraph
}
if(!reticulate::py_module_available("modularitypruning")){
  py_install("modularitypruning") # pip install modularitypruning
}
if(!reticulate::py_module_available("matplotlib")){
  py_install("matplotlib")
}
if(!reticulate::py_module_available("scipy")){
  py_install("scipy")
}
if(!reticulate::py_module_available("leidenalg")){
  py_install("leidenalg")
}
if(!reticulate::py_module_available("numpy")){
  py_install("numpy")
}
if(!reticulate::py_module_available("cairocffi")){
  py_install("cairocffi")
}
# load data
setwd("outputs")
t12_questionnaireCAMs <- readRDS(file = "t12_questionnaireCAMs.rds")
CAMfiles <- readRDS(file = "CAMfiles.rds")
CAMdrawn <- readRDS(file = "CAMdrawn.rds")


t12_questionnaireCAMs$total_min_prolific[t12_questionnaireCAMs$total_min_prolific > 1000] <- NA


# load functions
setwd("../../functions")
for(i in 1:length(dir())){
  # print(dir()[i])
  source(dir()[i], encoding = "utf-8")
}


setwd("../functions_CAMapp")
for(i in 1:length(dir())){
  # print(dir()[i])
  source(dir()[i], encoding = "utf-8")
}
rm(i)
setwd("..")
```


# load protocol

```{r protocol}
setwd("data")
protocolDataset <- "protocol_fixed_15.txt"
consider_Protocol <- TRUE

if(consider_Protocol){
  text <- readLines(protocolDataset, warn = FALSE)
  text <- readLines(textConnection(text, encoding="UTF-8"), encoding="UTF-8")
  
  if (testIfJson(file = text)) {
    protocol <- rjson::fromJSON(file = protocolDataset)
  } else{
    print("Invalid protocol uploaded")
  }
}

if(consider_Protocol){
  CAMfiles[[1]] <- CAMfiles[[1]][CAMfiles[[1]]$CAM %in% protocol$currentCAMs,]
  CAMfiles[[2]] <- CAMfiles[[2]][CAMfiles[[2]]$CAM %in% protocol$currentCAMs,]
  CAMfiles[[3]] <- CAMfiles[[3]][CAMfiles[[3]]$CAM.x %in% protocol$currentCAMs,]


  tmp_out <- overwriteTextNodes(protocolDat = protocol,
                                nodesDat = CAMfiles[[1]])
  CAMfiles[[1]] <- tmp_out[[1]]
  # tmp_out[[2]]
}

rm(protocolDataset, consider_Protocol, text, tmp_out)



tmp_text <- str_remove_all(string = CAMfiles[[1]]$text_summarized,
                           pattern = "_positive$|_negative$|_neutral$|_ambivalent$")
sort(unique(tmp_text))

CAMfiles[[1]]$text_summarized <- str_replace_all(string = CAMfiles[[1]]$text_summarized, pattern = " _", replacement = "_")

rm(tmp_text)

```


# split data

Data set is split according to country (Germany & USA) and persons with high conspiracy (3) and low conspiracy (1). 

## Country | not necessary

### CAMs USA
```{r}
setwd("outputs")
if(!file.exists("CAMs_USA")){
  dir.create("CAMs_USA")
}
setwd("CAMs_USA")


CAMfiles_USA <- CAMfiles

## check is ID data set is complete
if(!all(CAMfiles_USA[[1]]$participantCAM %in% t12_questionnaireCAMs$PROLIFIC_PID)){
    print("Error")
}else{
  tmp_ids <- t12_questionnaireCAMs$PROLIFIC_PID[t12_questionnaireCAMs$country.x == "USA"]
  
  ## keep only CAM data from USA
  CAMfiles_USA[[1]] <- CAMfiles_USA[[1]][CAMfiles_USA[[1]]$participantCAM %in% tmp_ids,]
  CAMfiles_USA[[2]] <- CAMfiles_USA[[2]][CAMfiles_USA[[2]]$participantCAM %in% tmp_ids,]
  CAMfiles_USA[[3]] <- CAMfiles_USA[[3]][CAMfiles_USA[[3]]$participantCAM.x %in% tmp_ids,]
  
  ## save files ob subsets
  vroom::vroom_write(x =  CAMfiles_USA[[1]], file = "CAM_nodes_USA.txt")
  vroom::vroom_write(x =  CAMfiles_USA[[2]], file = "CAM_connectors_USA.txt")
  vroom::vroom_write(x =  CAMfiles_USA[[3]], file = "CAM_merged_USA.txt")
}

```



### CAMs Germany
```{r}
setwd("outputs")
if(!file.exists("CAMs_Germany")){
  dir.create("CAMs_Germany")
}
setwd("CAMs_Germany")


CAMfiles_Germany <- CAMfiles

## check is ID data set is complete
if(!all(CAMfiles_Germany[[1]]$participantCAM %in% t12_questionnaireCAMs$PROLIFIC_PID)){
    print("Error")
}else{
  tmp_ids <- t12_questionnaireCAMs$PROLIFIC_PID[t12_questionnaireCAMs$country.x == "Germany"]
  
  ## keep only CAM data from Germany
  CAMfiles_Germany[[1]] <- CAMfiles_Germany[[1]][CAMfiles_Germany[[1]]$participantCAM %in% tmp_ids,]
  CAMfiles_Germany[[2]] <- CAMfiles_Germany[[2]][CAMfiles_Germany[[2]]$participantCAM %in% tmp_ids,]
  CAMfiles_Germany[[3]] <- CAMfiles_Germany[[3]][CAMfiles_Germany[[3]]$participantCAM.x %in% tmp_ids,]
  
  ## save files ob subsets
  vroom::vroom_write(x =  CAMfiles_Germany[[1]], file = "CAM_nodes_Germany.txt")
  vroom::vroom_write(x =  CAMfiles_Germany[[2]], file = "CAM_connectors_Germany.txt")
  vroom::vroom_write(x =  CAMfiles_Germany[[3]], file = "CAM_merged_Germany.txt")
}

```




## classes_conspiracy: 3 = high, 1 = low conspiracy

```{r}
### CAMs from high conspir belief

setwd("outputs")
if(!file.exists("CAMs_high")){
  dir.create("CAMs_high")
}
setwd("CAMs_high")


CAMfiles_high <- CAMfiles

## check is ID data set is complete
if(!all(CAMfiles_high[[1]]$participantCAM %in% t12_questionnaireCAMs$PROLIFIC_PID)){
    print("Error")
}else{
  tmp_ids <- t12_questionnaireCAMs$PROLIFIC_PID[t12_questionnaireCAMs$classes_conspiracy == 3]
  
  ## keep only CAM data from high conspir
  CAMfiles_high[[1]] <- CAMfiles_high[[1]][CAMfiles_high[[1]]$participantCAM %in% tmp_ids,]
  CAMfiles_high[[2]] <- CAMfiles_high[[2]][CAMfiles_high[[2]]$participantCAM %in% tmp_ids,]
  CAMfiles_high[[3]] <- CAMfiles_high[[3]][CAMfiles_high[[3]]$participantCAM.x %in% tmp_ids,]
  
  ## save files ob subsets
  # vroom::vroom_write(x =  CAMfiles_high[[1]], file = "CAM_nodes_high.txt")
  # vroom::vroom_write(x =  CAMfiles_high[[2]], file = "CAM_connectors_high.txt")
  # vroom::vroom_write(x =  CAMfiles_high[[3]], file = "CAM_merged_high.txt")
}

# saveRDS(CAMfiles_high, file = "CAMfiles_high.RDS")
rm(tmp_ids)


### CAMs from low conspir belief

if(!file.exists("CAMs_low")){
  dir.create("CAMs_low")
}
setwd("CAMs_low")


CAMfiles_low <- CAMfiles

## check is ID data set is complete
if(!all(CAMfiles_low[[1]]$participantCAM %in% t12_questionnaireCAMs$PROLIFIC_PID)){
    print("Error")
}else{
  tmp_ids <- t12_questionnaireCAMs$PROLIFIC_PID[t12_questionnaireCAMs$classes_conspiracy == 1]
  
  ## keep only CAM data from low conspir
  CAMfiles_low[[1]] <- CAMfiles_low[[1]][CAMfiles_low[[1]]$participantCAM %in% tmp_ids,]
  CAMfiles_low[[2]] <- CAMfiles_low[[2]][CAMfiles_low[[2]]$participantCAM %in% tmp_ids,]
  CAMfiles_low[[3]] <- CAMfiles_low[[3]][CAMfiles_low[[3]]$participantCAM.x %in% tmp_ids,]
  
  ## save files ob subsets
  # vroom::vroom_write(x =  CAMfiles_low[[1]], file = "CAM_nodes_low.txt")
  # vroom::vroom_write(x =  CAMfiles_low[[2]], file = "CAM_connectors_low.txt")
  # vroom::vroom_write(x =  CAMfiles_low[[3]], file = "CAM_merged_low.txt")
}

# saveRDS(CAMfiles_low, file = "CAMfiles_low.RDS")
rm(tmp_ids)

setwd("..")

```


#.
# Analyze questionnaire data

## t1

### preparation

```{r}
t1_table <- data.frame(
  Questionnaire = c(
    "GCB",
    "CMQ",
    "CCS-Q",
    "CRKQ"
  ),
  M = c(
    round(mean(t12_questionnaireCAMs$mean_GCB),2),
    round(mean(t12_questionnaireCAMs$mean_CMQ),2),
    round(mean(t12_questionnaireCAMs$mean_CCSQ),2),
    round(mean(t12_questionnaireCAMs$mean_CRKQ),2)
  ),
  SD = c(
    round(sd(t12_questionnaireCAMs$mean_GCB),2),
    round(sd(t12_questionnaireCAMs$mean_CMQ),2),
    round(sd(t12_questionnaireCAMs$mean_CCSQ),2),
    round(sd(t12_questionnaireCAMs$mean_CRKQ),2)
  )
)

t1_table

```

```{r}
t12_questionnaireCAMs_high <- t12_questionnaireCAMs[t12_questionnaireCAMs$classes_conspiracy == 3, ]
t12_questionnaireCAMs_low <- t12_questionnaireCAMs[t12_questionnaireCAMs$classes_conspiracy == 1, ]

t12_questionnaireCAMs$classes_conspiracy <- as.character(t12_questionnaireCAMs$classes_conspiracy)
```

### t-Tests prerequisites

```{r}
# GCB
leveneTest(mean_GCB ~ classes_conspiracy, data = t12_questionnaireCAMs, center = "median")
  # -> Welch-Test


# GCB
leveneTest(mean_CMQ ~ classes_conspiracy, data = t12_questionnaireCAMs, center = "median")
  # -> Welch-Test


# CCS-Q
leveneTest(mean_CCSQ ~ classes_conspiracy, data = t12_questionnaireCAMs, center = "median")
  # -> Welch-Test


# CRKQ
leveneTest(mean_CRKQ ~ classes_conspiracy, data = t12_questionnaireCAMs, center = "median")
  # -> Welch-Test


```


### t1 table

```{r}
t1_table <- data.frame(
  Questionnaire = c(
    "GCB",
    "CMQ",
    "CCS-Q",
    "CRKQ"
  ),
  M_o = c(
    round(mean(t12_questionnaireCAMs$mean_GCB),2),
    round(mean(t12_questionnaireCAMs$mean_CMQ),2),
    round(mean(t12_questionnaireCAMs$mean_CCSQ),2),
    round(mean(t12_questionnaireCAMs$mean_CRKQ),2)
  ),
  
  SD_o = c(
    round(sd(t12_questionnaireCAMs$mean_GCB),2),
    round(sd(t12_questionnaireCAMs$mean_CMQ),2),
    round(sd(t12_questionnaireCAMs$mean_CCSQ),2),
    round(sd(t12_questionnaireCAMs$mean_CRKQ),2)
  ),
  
  M_h = c(
    round(mean(t12_questionnaireCAMs_high$mean_GCB),2),
    round(mean(t12_questionnaireCAMs_high$mean_CMQ),2),
    round(mean(t12_questionnaireCAMs_high$mean_CCSQ),2),
    round(mean(t12_questionnaireCAMs_high$mean_CRKQ),2)
  ),
  
  SD_h = c(
    round(sd(t12_questionnaireCAMs_high$mean_GCB),2),
    round(sd(t12_questionnaireCAMs_high$mean_CMQ),2),
    round(sd(t12_questionnaireCAMs_high$mean_CCSQ),2),
    round(sd(t12_questionnaireCAMs_high$mean_CRKQ),2)
  ),
  
  M_l = c(
    round(mean(t12_questionnaireCAMs_low$mean_GCB),2),
    round(mean(t12_questionnaireCAMs_low$mean_CMQ),2),
    round(mean(t12_questionnaireCAMs_low$mean_CCSQ),2),
    round(mean(t12_questionnaireCAMs_low$mean_CRKQ),2)
  ),
  
  SD_l = c(
    round(sd(t12_questionnaireCAMs_low$mean_GCB),2),
    round(sd(t12_questionnaireCAMs_low$mean_CMQ),2),
    round(sd(t12_questionnaireCAMs_low$mean_CCSQ),2),
    round(sd(t12_questionnaireCAMs_low$mean_CRKQ),2)
  ),
  
  t = c(
    round(t.test(
        t12_questionnaireCAMs_high$mean_GCB,
        t12_questionnaireCAMs_low$mean_GCB,
        var.equal = FALSE, alternative = "two.sided"
      )$statistic, 2),
    round(t.test(
        t12_questionnaireCAMs_high$mean_CMQ,
        t12_questionnaireCAMs_low$mean_CMQ,
        var.equal = FALSE, alternative = "two.sided"
      )$statistic, 2),
    round(t.test(
        t12_questionnaireCAMs_high$mean_CCSQ,
        t12_questionnaireCAMs_low$mean_CCSQ,
        var.equal = FALSE, alternative = "two.sided"
      )$statistic, 2),
    round(t.test(
        t12_questionnaireCAMs_high$mean_CRKQ,
        t12_questionnaireCAMs_low$mean_CRKQ,
        var.equal = FALSE, alternative = "two.sided"
      )$statistic, 2)
  ),
  
  df = c(
    round(t.test(
        t12_questionnaireCAMs_high$mean_GCB,
        t12_questionnaireCAMs_low$mean_GCB,
        var.equal = FALSE, alternative = "two.sided"
      )$parameter, 2),
    round(t.test(
        t12_questionnaireCAMs_high$mean_CMQ,
        t12_questionnaireCAMs_low$mean_CMQ,
        var.equal = FALSE, alternative = "two.sided"
      )$parameter, 2),
    round(t.test(
        t12_questionnaireCAMs_high$mean_CCSQ,
        t12_questionnaireCAMs_low$mean_CCSQ,
        var.equal = FALSE, alternative = "two.sided"
      )$parameter, 2),
    round(t.test(
        t12_questionnaireCAMs_high$mean_CRKQ,
        t12_questionnaireCAMs_low$mean_CRKQ,
        var.equal = FALSE, alternative = "two.sided"
      )$parameter, 2)
  ),
  
  p = c(
    round(t.test(
        t12_questionnaireCAMs_high$mean_GCB,
        t12_questionnaireCAMs_low$mean_GCB,
        var.equal = FALSE, alternative = "two.sided"
      )$p.value, 3),
    round(t.test(
        t12_questionnaireCAMs_high$mean_CMQ,
        t12_questionnaireCAMs_low$mean_CMQ,
        var.equal = FALSE, alternative = "two.sided"
      )$p.value, 3),
    round(t.test(
        t12_questionnaireCAMs_high$mean_CCSQ,
        t12_questionnaireCAMs_low$mean_CCSQ,
        var.equal = FALSE, alternative = "two.sided"
      )$p.value, 3),
    round(t.test(
        t12_questionnaireCAMs_high$mean_CRKQ,
        t12_questionnaireCAMs_low$mean_CRKQ,
        var.equal = FALSE, alternative = "two.sided"
      )$p.value, 3)
  ),
  
  Cohen_d = c(
    round(cohensD(t12_questionnaireCAMs_high$mean_GCB,
                  t12_questionnaireCAMs_low$mean_GCB), 2),
    round(cohensD(t12_questionnaireCAMs_high$mean_CMQ,
                  t12_questionnaireCAMs_low$mean_CMQ), 2),
    round(cohensD(t12_questionnaireCAMs_high$mean_CCSQ,
                  t12_questionnaireCAMs_low$mean_CCSQ), 2),
    round(cohensD(t12_questionnaireCAMs_high$mean_CRKQ,
                  t12_questionnaireCAMs_low$mean_CRKQ), 2)
  )
)

t1_table

```


## t2

### preparation

```{r}
t12_questionnaireCAMs_onlyt2 <- t12_questionnaireCAMs %>%
  filter(PROLIFIC_PID %in% CAMfiles[[1]][["participantCAM"]])

t12_questionnaireCAMs_high <- t12_questionnaireCAMs_onlyt2[t12_questionnaireCAMs_onlyt2$classes_conspiracy == 3, ]
t12_questionnaireCAMs_low <- t12_questionnaireCAMs_onlyt2[t12_questionnaireCAMs_onlyt2$classes_conspiracy == 1, ]

t12_questionnaireCAMs_onlyt2$classes_conspiracy <- as.character(t12_questionnaireCAMs_onlyt2$classes_conspiracy)

```


### t-Tests prerequisites

```{r}
# GCB
leveneTest(mean_GCB ~ classes_conspiracy, data = t12_questionnaireCAMs_onlyt2, center = "median")
  # -> t-Test


# GCB
leveneTest(mean_CMQ ~ classes_conspiracy, data = t12_questionnaireCAMs_onlyt2, center = "median")
  # -> t-Test


# CCS-Q
leveneTest(mean_CCSQ ~ classes_conspiracy, data = t12_questionnaireCAMs_onlyt2, center = "median")
  # -> Welch-Test


# CRKQ
leveneTest(mean_CRKQ ~ classes_conspiracy, data = t12_questionnaireCAMs_onlyt2, center = "median")
  # -> Welch-Test


```


### t2 table

```{r}
t2_table <- data.frame(
  Questionnaire = c(
    "GCB",
    "CMQ",
    "CCS-Q",
    "CRKQ"
  ),
  M_o = c(
    round(mean(t12_questionnaireCAMs_onlyt2$mean_GCB),2),
    round(mean(t12_questionnaireCAMs_onlyt2$mean_CMQ),2),
    round(mean(t12_questionnaireCAMs_onlyt2$mean_CCSQ),2),
    round(mean(t12_questionnaireCAMs_onlyt2$mean_CRKQ),2)
  ),
  
  SD_o = c(
    round(sd(t12_questionnaireCAMs_onlyt2$mean_GCB),2),
    round(sd(t12_questionnaireCAMs_onlyt2$mean_CMQ),2),
    round(sd(t12_questionnaireCAMs_onlyt2$mean_CCSQ),2),
    round(sd(t12_questionnaireCAMs_onlyt2$mean_CRKQ),2)
  ),
  
  M_h = c(
    round(mean(t12_questionnaireCAMs_high$mean_GCB),2),
    round(mean(t12_questionnaireCAMs_high$mean_CMQ),2),
    round(mean(t12_questionnaireCAMs_high$mean_CCSQ),2),
    round(mean(t12_questionnaireCAMs_high$mean_CRKQ),2)
  ),
  
  SD_h = c(
    round(sd(t12_questionnaireCAMs_high$mean_GCB),2),
    round(sd(t12_questionnaireCAMs_high$mean_CMQ),2),
    round(sd(t12_questionnaireCAMs_high$mean_CCSQ),2),
    round(sd(t12_questionnaireCAMs_high$mean_CRKQ),2)
  ),
  
  M_l = c(
    round(mean(t12_questionnaireCAMs_low$mean_GCB),2),
    round(mean(t12_questionnaireCAMs_low$mean_CMQ),2),
    round(mean(t12_questionnaireCAMs_low$mean_CCSQ),2),
    round(mean(t12_questionnaireCAMs_low$mean_CRKQ),2)
  ),
  
  SD_l = c(
    round(sd(t12_questionnaireCAMs_low$mean_GCB),2),
    round(sd(t12_questionnaireCAMs_low$mean_CMQ),2),
    round(sd(t12_questionnaireCAMs_low$mean_CCSQ),2),
    round(sd(t12_questionnaireCAMs_low$mean_CRKQ),2)
  ),
  
  t = c(
    round(t.test(
        t12_questionnaireCAMs_high$mean_GCB,
        t12_questionnaireCAMs_low$mean_GCB,
        var.equal = TRUE, alternative = "two.sided"
      )$statistic, 2),
    round(t.test(
        t12_questionnaireCAMs_high$mean_CMQ,
        t12_questionnaireCAMs_low$mean_CMQ,
        var.equal = TRUE, alternative = "two.sided"
      )$statistic, 2),
    round(t.test(
        t12_questionnaireCAMs_high$mean_CCSQ,
        t12_questionnaireCAMs_low$mean_CCSQ,
        var.equal = FALSE, alternative = "two.sided"
      )$statistic, 2),
    round(t.test(
        t12_questionnaireCAMs_high$mean_CRKQ,
        t12_questionnaireCAMs_low$mean_CRKQ,
        var.equal = FALSE, alternative = "two.sided"
      )$statistic, 2)
  ),
  
  df = c(
    round(t.test(
        t12_questionnaireCAMs_high$mean_GCB,
        t12_questionnaireCAMs_low$mean_GCB,
        var.equal = TRUE, alternative = "two.sided"
      )$parameter, 2),
    round(t.test(
        t12_questionnaireCAMs_high$mean_CMQ,
        t12_questionnaireCAMs_low$mean_CMQ,
        var.equal = TRUE, alternative = "two.sided"
      )$parameter, 2),
    round(t.test(
        t12_questionnaireCAMs_high$mean_CCSQ,
        t12_questionnaireCAMs_low$mean_CCSQ,
        var.equal = FALSE, alternative = "two.sided"
      )$parameter, 2),
    round(t.test(
        t12_questionnaireCAMs_high$mean_CRKQ,
        t12_questionnaireCAMs_low$mean_CRKQ,
        var.equal = FALSE, alternative = "two.sided"
      )$parameter, 2)
  ),
  
  p = c(
    round(t.test(
        t12_questionnaireCAMs_high$mean_GCB,
        t12_questionnaireCAMs_low$mean_GCB,
        var.equal = TRUE, alternative = "two.sided"
      )$p.value, 3),
    round(t.test(
        t12_questionnaireCAMs_high$mean_CMQ,
        t12_questionnaireCAMs_low$mean_CMQ,
        var.equal = TRUE, alternative = "two.sided"
      )$p.value, 3),
    round(t.test(
        t12_questionnaireCAMs_high$mean_CCSQ,
        t12_questionnaireCAMs_low$mean_CCSQ,
        var.equal = FALSE, alternative = "two.sided"
      )$p.value, 3),
    round(t.test(
        t12_questionnaireCAMs_high$mean_CRKQ,
        t12_questionnaireCAMs_low$mean_CRKQ,
        var.equal = FALSE, alternative = "two.sided"
      )$p.value, 3)
  ),
  
  Cohen_d = c(
    round(cohensD(t12_questionnaireCAMs_high$mean_GCB,
                  t12_questionnaireCAMs_low$mean_GCB), 2),
    round(cohensD(t12_questionnaireCAMs_high$mean_CMQ,
                  t12_questionnaireCAMs_low$mean_CMQ), 2),
    round(cohensD(t12_questionnaireCAMs_high$mean_CCSQ,
                  t12_questionnaireCAMs_low$mean_CCSQ), 2),
    round(cohensD(t12_questionnaireCAMs_high$mean_CRKQ,
                  t12_questionnaireCAMs_low$mean_CRKQ), 2)
  )
)

t2_table

```


#.
# Analyze CAM data

## Network Indicators

https://camtools-documentation.readthedocs.io/en/master/CAM-App/#compute-network-indicators

- num_nodes_macro: Number of concepts.
- num_nodes_pos_macro: Number of concepts.       
- num_nodes_neg_macro: Number of negative concepts.
- num_nodes_neut_macro: Number of neutral concepts.
- num_nodes_ambi_macro: Number of ambivalent concepts.
- mean_valence_macro: The mean valence of all concepts.
- density_macro: The density of a CAM refers to the ratio between the actual number of edges and the maximum possible number of edges in the CAM.
- num_edges_macro: Number of edges. 
- transitivity_macro: Transitivity measures the likelihood that the neighboring vertices of a vertex are connected. This is also known as the clustering coefficient.
- assortativity_valence_macro: The coefficient ranges from [-1, 1], whereby positive values indicate that concepts with the same valence have a tendency to connect with each other (level of homophily). The value -1 indicates a completely disassortative CAM.


### Preparation

```{r}
# high
CAMdrawn_high <- draw_CAM(dat_merged = CAMfiles_high[[3]],
                          dat_nodes = CAMfiles_high[[1]],
                          ids_CAMs = "all", plot_CAM = FALSE, useCoordinates = TRUE,
                          relvertexsize = 5,
                          reledgesize = 1)

netIndicators_high <- compute_indicatorsCAM(drawn_CAM = CAMdrawn_high)
netIndicators_high$group <- "high"



# low
CAMdrawn_low <- draw_CAM(dat_merged = CAMfiles_low[[3]],
                         dat_nodes = CAMfiles_low[[1]],
                         ids_CAMs = "all", plot_CAM = FALSE, useCoordinates = TRUE,
                         relvertexsize = 5,
                         reledgesize = 1)

netIndicators_low <- compute_indicatorsCAM(drawn_CAM = CAMdrawn_low)
netIndicators_low$group <- "low"


# overall
CAMdrawn_overall <- draw_CAM(dat_merged = CAMfiles[[3]],
                          dat_nodes = CAMfiles[[1]],
                          ids_CAMs = "all", plot_CAM = FALSE, useCoordinates = TRUE,
                          relvertexsize = 5,
                          reledgesize = 1)

netIndicators_overall <- rbind(netIndicators_high, netIndicators_low)

```


### Pie Charts

#### Overall

```{r}
data <- data.frame(
  pos = sum(netIndicators_overall$num_nodes_pos_macro),
  neg = sum(netIndicators_overall$num_nodes_neg_macro),
  neut = sum(netIndicators_overall$num_nodes_neut_macro),
  ambi = sum(netIndicators_overall$num_nodes_ambi_macro)
)

percentages <- round(prop.table(as.matrix(data), margin = 1) * 100, 1)

df <- data.frame(
  variable = colnames(data),
  value = unlist(data),
  percent = unlist(percentages)
)

variable_labels <- c("pos" = "positive", "neg" = "negative", "neut" = "neutral", "ambi" = "ambivalent")
df$variable <- factor(df$variable, levels = names(variable_labels))
df$variable_label <- variable_labels[df$variable]

pie_overall <- ggplot(df, aes(x = "", y = value, fill = variable, label = paste0(percentages, "%"))) +
  geom_bar(stat = "identity", color = "black", width = 1) +
  coord_polar("y", start = 0) +
  geom_text(position = position_stack(vjust = 0.5), size = 4) +
  scale_fill_manual(values = c("pos" = "green", "neg" = "red", "neut" = "yellow", "ambi" = "purple"), name = "Concept type:", labels = variable_labels) +
  theme_void() + 
  theme(
    legend.position = "right"
  )

print(pie_overall)

rm(df, percentages, data, pie_overall, variable_labels)

```


#### high

```{r}
data <- data.frame(
  pos = sum(netIndicators_high$num_nodes_pos_macro),
  neg = sum(netIndicators_high$num_nodes_neg_macro),
  neut = sum(netIndicators_high$num_nodes_neut_macro),
  ambi = sum(netIndicators_high$num_nodes_ambi_macro)
)

percentages <- round(prop.table(as.matrix(data), margin = 1) * 100, 1)

df <- data.frame(
  variable = colnames(data),
  value = unlist(data),
  percent = unlist(percentages)
)

variable_labels <- c("pos" = "positive", "neg" = "negative", "neut" = "neutral", "ambi" = "ambivalent")
df$variable <- factor(df$variable, levels = names(variable_labels))
df$variable_label <- variable_labels[df$variable]

pie_high <- ggplot(df, aes(x = "", y = value, fill = variable, label = paste0(percentages, "%"))) +
  geom_bar(stat = "identity", color = "black", width = 1) +
  coord_polar("y", start = 0) +
  geom_text(position = position_stack(vjust = 0.5), size = 4) +
  scale_fill_manual(values = c("pos" = "green", "neg" = "red", "neut" = "yellow", "ambi" = "purple"), name = "Concept type:", labels = variable_labels) +
  theme_void() +
  theme(
    legend.position = "right"
  )

print(pie_high)

rm(df, percentages, data, pie_high, variable_labels)

```


#### low

```{r}
data <- data.frame(
  pos = sum(netIndicators_low$num_nodes_pos_macro),
  neg = sum(netIndicators_low$num_nodes_neg_macro),
  neut = sum(netIndicators_low$num_nodes_neut_macro),
  ambi = sum(netIndicators_low$num_nodes_ambi_macro)
)

percentages <- round(prop.table(as.matrix(data), margin = 1) * 100, 1)

df <- data.frame(
  variable = colnames(data),
  value = unlist(data),
  percent = unlist(percentages)
)

variable_labels <- c("pos" = "positive", "neg" = "negative", "neut" = "neutral", "ambi" = "ambivalent")
df$variable <- factor(df$variable, levels = names(variable_labels))
df$variable_label <- variable_labels[df$variable]

pie_low <- ggplot(df, aes(x = "", y = value, fill = variable, label = paste0(percentages, "%"))) +
  geom_bar(stat = "identity", color = "black", width = 1) +
  coord_polar("y", start = 0) +
  geom_text(position = position_stack(vjust = 0.5), size = 4) +
  scale_fill_manual(values = c("pos" = "green", "neg" = "red", "neut" = "yellow", "ambi" = "purple"), name = "Concept type:", labels = variable_labels) +
  theme_void() +
  theme(
    legend.position = "right"
  )

print(pie_low)

rm(df, percentages, data, pie_low, variable_labels)

```


### t-Tests prerequisites

```{r}
# number of concepts
leveneTest(num_nodes_macro ~ group, data = netIndicators_overall, center = "median")
  # -> t-Test


# number of positive concepts
leveneTest(num_nodes_pos_macro ~ group, data = netIndicators_overall, center = "median")
  # -> t-Test


# number of negative concepts
leveneTest(num_nodes_neg_macro ~ group, data = netIndicators_overall, center = "median")
  # -> t-Test


# number of neutral concepts
leveneTest(num_nodes_neut_macro ~ group, data = netIndicators_overall, center = "median")
  # -> t-Test


# number of ambivalent concepts
leveneTest(num_nodes_ambi_macro ~ group, data = netIndicators_overall, center = "median")
  # -> t-Test


# mean valence
leveneTest(mean_valence_macro ~ group, data = netIndicators_overall, center = "median")
  # -> t-Test


# number of edges
leveneTest(num_edges_macro ~ group, data = netIndicators_overall, center = "median")
  # -> t-Test


# density
leveneTest(density_macro ~ group, data = netIndicators_overall, center = "median")
  # -> t-Test


# transitivity
leveneTest(transitivity_macro ~ group, data = netIndicators_overall, center = "median")
  # -> t-Test


# assortativity
leveneTest(assortativity_valence_macro ~ group, data = netIndicators_overall, center = "median")
  # -> t-Test

```


### Table of Network Indicators

```{r}
netIndicators_table <- data.frame(
  indicator = c(
    "n",
    "number of concepts",
    "positive concepts",
    "negative concepts",
    "neutral concepts",
    "ambivalent concepts",
    "mean valence",
    "number of edges",
    "density",
    "transitivity",
    "assortativity valence"
  ),
  
  overall = c(
    nrow(netIndicators_overall),
    round(mean(netIndicators_overall$num_nodes_macro), 2),
    round(mean(netIndicators_overall$num_nodes_pos_macro), 2),
    round(mean(netIndicators_overall$num_nodes_neg_macro), 2),
    round(mean(netIndicators_overall$num_nodes_neut_macro), 2),
    round(mean(netIndicators_overall$num_nodes_ambi_macro), 2),
    round(mean(netIndicators_overall$mean_valence_macro), 2),
    round(mean(netIndicators_overall$num_edges_macro), 2),
    round(mean(netIndicators_overall$density_macro), 2),
    round(mean(netIndicators_overall$transitivity_macro), 2),
    round(mean(netIndicators_overall$assortativity_valence_macro, na.rm = TRUE), 2)
  ),
  
  SD_o = c(
    "-",
    round(sd(netIndicators_overall$num_nodes_macro), 2),
    round(sd(netIndicators_overall$num_nodes_pos_macro), 2),
    round(sd(netIndicators_overall$num_nodes_neg_macro), 2),
    round(sd(netIndicators_overall$num_nodes_neut_macro), 2),
    round(sd(netIndicators_overall$num_nodes_ambi_macro), 2),
    round(sd(netIndicators_overall$mean_valence_macro), 2),
    round(sd(netIndicators_overall$num_edges_macro), 2),
    round(sd(netIndicators_overall$density_macro), 2),
    round(sd(netIndicators_overall$transitivity_macro), 2),
    round(sd(netIndicators_overall$assortativity_valence_macro, na.rm = TRUE), 2)
  ),
  
  high = c(
    nrow(netIndicators_high),
    round(mean(netIndicators_high$num_nodes_macro), 2),
    round(mean(netIndicators_high$num_nodes_pos_macro), 2),
    round(mean(netIndicators_high$num_nodes_neg_macro), 2),
    round(mean(netIndicators_high$num_nodes_neut_macro), 2),
    round(mean(netIndicators_high$num_nodes_ambi_macro), 2),
    round(mean(netIndicators_high$mean_valence_macro), 2),
    round(mean(netIndicators_high$num_edges_macro), 2),
    round(mean(netIndicators_high$density_macro), 2),
    round(mean(netIndicators_high$transitivity_macro), 2),
    round(mean(netIndicators_high$assortativity_valence_macro, na.rm = TRUE), 2)
  ),
  
  SD_h = c(
    "-",
    round(sd(netIndicators_high$num_nodes_macro), 2),
    round(sd(netIndicators_high$num_nodes_pos_macro), 2),
    round(sd(netIndicators_high$num_nodes_neg_macro), 2),
    round(sd(netIndicators_high$num_nodes_neut_macro), 2),
    round(sd(netIndicators_high$num_nodes_ambi_macro), 2),
    round(sd(netIndicators_high$mean_valence_macro), 2),
    round(sd(netIndicators_high$num_edges_macro), 2),
    round(sd(netIndicators_high$density_macro), 2),
    round(sd(netIndicators_high$transitivity_macro), 2),
    round(sd(netIndicators_high$assortativity_valence_macro, na.rm = TRUE), 2)
  ),
  
  low = c(
    nrow(netIndicators_low),
    round(mean(netIndicators_low$num_nodes_macro), 2),
    round(mean(netIndicators_low$num_nodes_pos_macro), 2),
    round(mean(netIndicators_low$num_nodes_neg_macro), 2),
    round(mean(netIndicators_low$num_nodes_neut_macro), 2),
    round(mean(netIndicators_low$num_nodes_ambi_macro), 2),
    round(mean(netIndicators_low$mean_valence_macro), 2),
    round(mean(netIndicators_low$num_edges_macro), 2),
    round(mean(netIndicators_low$density_macro), 2),
    round(mean(netIndicators_low$transitivity_macro), 2),
    round(mean(netIndicators_low$assortativity_valence_macro, na.rm = TRUE), 2)
  ),
  
  SD_l = c(
    "-",
    round(sd(netIndicators_low$num_nodes_macro), 2),
    round(sd(netIndicators_low$num_nodes_pos_macro), 2),
    round(sd(netIndicators_low$num_nodes_neg_macro), 2),
    round(sd(netIndicators_low$num_nodes_neut_macro), 2),
    round(sd(netIndicators_low$num_nodes_ambi_macro), 2),
    round(sd(netIndicators_low$mean_valence_macro), 2),
    round(sd(netIndicators_low$num_edges_macro), 2),
    round(sd(netIndicators_low$density_macro), 2),
    round(sd(netIndicators_low$transitivity_macro), 2),
    round(sd(netIndicators_low$assortativity_valence_macro, na.rm = TRUE), 2)
  ),
  
  t.119 = c(
    "-",
    round(t.test(
        netIndicators_high$num_nodes_macro,
        netIndicators_low$num_nodes_macro,
        var.equal = TRUE, alternative = "two.sided"
      )$statistic, 2),
    round(t.test(
        netIndicators_high$num_nodes_pos_macro,
        netIndicators_low$num_nodes_pos_macro,
        var.equal = TRUE, alternative = "two.sided"
      )$statistic, 2),
    round(t.test(
        netIndicators_high$num_nodes_neg_macro,
        netIndicators_low$num_nodes_neg_macro,
        var.equal = TRUE, alternative = "two.sided"
      )$statistic, 2),
    round(t.test(
        netIndicators_high$num_nodes_neut_macro,
        netIndicators_low$num_nodes_neut_macro,
        var.equal = TRUE,
        alternative = "two.sided"
      )$statistic, 2),
    round(t.test(
        netIndicators_high$num_nodes_ambi_macro,
        netIndicators_low$num_nodes_ambi_macro,
        var.equal = TRUE, alternative = "two.sided"
      )$statistic, 2),
    round(t.test(
        netIndicators_high$mean_valence_macro,
        netIndicators_low$mean_valence_macro,
        var.equal = TRUE, alternative = "two.sided"
      )$statistic, 2),
    round(t.test(
        netIndicators_high$num_edges_macro,
        netIndicators_low$num_edges_macro,
        var.equal = TRUE, alternative = "two.sided"
      )$statistic, 2),
    round(t.test(
        netIndicators_high$density_macro,
        netIndicators_low$density_macro,
        var.equal = TRUE, alternative = "two.sided"
      )$statistic, 2),
    round(t.test(
        netIndicators_high$transitivity_macro,
        netIndicators_low$transitivity_macro,
        var.equal = TRUE, alternative = "two.sided"
      )$statistic, 2),
    round(t.test(
        netIndicators_high$assortativity_valence_macro,
        netIndicators_low$assortativity_valence_macro,
        var.equal = TRUE, alternative = "two.sided"
      )$statistic, 2)
  ),
  
  p = c(
    "-",
    round(t.test(
        netIndicators_high$num_nodes_macro,
        netIndicators_low$num_nodes_macro,
        var.equal = TRUE, alternative = "two.sided"
      )$p.value, 3),
    round(t.test(
        netIndicators_high$num_nodes_pos_macro,
        netIndicators_low$num_nodes_pos_macro,
        var.equal = TRUE, alternative = "two.sided"
      )$p.value, 3),
    round(t.test(
        netIndicators_high$num_nodes_neg_macro,
        netIndicators_low$num_nodes_neg_macro,
        var.equal = TRUE, alternative = "two.sided"
      )$p.value, 3),
    round(t.test(
        netIndicators_high$num_nodes_neut_macro,
        netIndicators_low$num_nodes_neut_macro,
        var.equal = TRUE,
        alternative = "two.sided"
      )$p.value, 3),
    round(t.test(
        netIndicators_high$num_nodes_ambi_macro,
        netIndicators_low$num_nodes_ambi_macro,
        var.equal = TRUE, alternative = "two.sided"
      )$p.value, 3),
    round(t.test(
        netIndicators_high$mean_valence_macro,
        netIndicators_low$mean_valence_macro,
        var.equal = TRUE, alternative = "two.sided"
      )$p.value, 3),
    round(t.test(
        netIndicators_high$num_nodes_macro,
        netIndicators_low$num_nodes_macro,
        var.equal = TRUE, alternative = "two.sided"
      )$p.value, 3),
    round(t.test(
        netIndicators_high$density_macro,
        netIndicators_low$density_macro,
        var.equal = TRUE, alternative = "two.sided"
      )$p.value, 3),
    round(t.test(
        netIndicators_high$transitivity_macro,
        netIndicators_low$transitivity_macro,
        var.equal = TRUE, alternative = "two.sided"
      )$p.value, 3),
    round(t.test(
        netIndicators_high$assortativity_valence_macro,
        netIndicators_low$assortativity_valence_macro,
        var.equal = TRUE, alternative = "two.sided"
      )$p.value, 3)
  ),
  Cohen_d = c(
    "-",
    "-",
    "-",
    round(cohensD(netIndicators_high$num_nodes_neg_macro,
                  netIndicators_low$num_nodes_neg_macro), 2),
    "-",
    "-",
    round(cohensD(netIndicators_high$mean_valence_macro,
                  netIndicators_low$mean_valence_macro), 2),
    "-",
    "-",
    "-",
    "-"
  )
)

netIndicators_table

```


## Concepts (Word lists)

### concepts -> categories (split)

```{r}
setwd("outputs")

# overall
nodes <- CAMfiles[[1]]

## Text in different columns
nodes_summarized_overall_split <- nodes[, c("text_summarized", "text")] %>%
  group_by(text_summarized) %>%
  mutate(row_num = row_number()) %>%
  spread(key = row_num, value = text) %>%
  rename_with(~paste("element", .), -text_summarized) %>%
  ungroup() %>%
  select(text_summarized, everything())

# Text in the same column
# nodes_summarized2_split <- nodes[, c("text_summarized", "text")] %>%
#   group_by(text_summarized) %>%
#   summarise(text = paste(text, collapse = ", ")) %>%
#   ungroup()

write.xlsx(nodes_summarized_overall_split, file = "nodes_summarized_overall_split.xlsx")

# high
nodes <- CAMfiles_high[[1]]

## Text in different columns
nodes_summarized_high_split <- nodes[, c("text_summarized", "text")] %>%
  group_by(text_summarized) %>%
  mutate(row_num = row_number()) %>%
  spread(key = row_num, value = text) %>%
  rename_with(~paste("element", .), -text_summarized) %>%
  ungroup() %>%
  select(text_summarized, everything())

write.xlsx(nodes_summarized_high_split, file = "nodes_summarized_high_split.xlsx")

# low
nodes <- CAMfiles_low[[1]]

## Text in different columns
nodes_summarized_low_split <- nodes[, c("text_summarized", "text")] %>%
  group_by(text_summarized) %>%
  mutate(row_num = row_number()) %>%
  spread(key = row_num, value = text) %>%
  rename_with(~paste("element", .), -text_summarized) %>%
  ungroup() %>%
  select(text_summarized, everything())


write.xlsx(nodes_summarized_low_split, file = "nodes_summarized_low_split.xlsx")
setwd("..")

rm(nodes)
```


### concepts -> categories 

```{r}
nodes <- CAMfiles[[1]]
nodes$text_summarized <- gsub("_positive|_negative|_neutral|_ambivalent", "", nodes$text_summarized)

# Text in different columns
nodes_summarized <- nodes[, c("text_summarized", "text")] %>%
  group_by(text_summarized) %>%
  mutate(row_num = row_number()) %>%
  spread(key = row_num, value = text) %>%
  rename_with(~paste("element", .), -text_summarized) %>%
  ungroup() %>%
  select(text_summarized, everything())

# Text in the same column
nodes_summarized2 <- nodes[, c("text_summarized", "text")] %>%
  group_by(text_summarized) %>%
  summarise(text = paste(text, collapse = ", ")) %>%
  ungroup()

setwd("outputs")
write.xlsx(nodes_summarized2, file = "nodes_summarized2.xlsx")
setwd("..")

rm(nodes)
```


### split

#### Preparation high & low

```{r}
# Word list high
tmp_text <- str_remove_all(string = CAMfiles_high[[1]]$text,
                           pattern = "_positive$|_negative$|_neutral$|_ambivalent$")

CAMwordlist_high_split <- create_wordlist(
  dat_nodes = CAMfiles_high[[1]],
  dat_merged = CAMfiles_high[[3]],
  useSummarized = TRUE,
  order = "frequency",
  splitByValence = TRUE,
  comments = TRUE,
  raterSubsetWords = NULL,
  rater = FALSE
)

dat_nodes <- CAMfiles_high[[1]]
dat_nodes$text <-   dat_nodes$text_summarized
sum(stringr::str_detect(string = dat_nodes$text, pattern = "_positive$|_negative$|_neutral$|_ambivalent$")) < nrow(dat_nodes)


# Word list low
tmp_text <- str_remove_all(string = CAMfiles_low[[1]]$text,
                           pattern = "_positive$|_negative$|_neutral$|_ambivalent$")

CAMwordlist_low_split <- create_wordlist(
  dat_nodes = CAMfiles_low[[1]],
  dat_merged = CAMfiles_low[[3]],
  useSummarized = TRUE,
  order = "frequency",
  splitByValence = TRUE,
  comments = TRUE,
  raterSubsetWords = NULL,
  rater = FALSE
)

dat_nodes <- CAMfiles_low[[1]]
dat_nodes$text <-   dat_nodes$text_summarized
sum(stringr::str_detect(string = dat_nodes$text, pattern = "_positive$|_negative$|_neutral$|_ambivalent$")) < nrow(dat_nodes)

rm(tmp_text)

```


#### Concept Comparisons 

```{r}

# setwd("outputs/CAMwordlists")
# write.xlsx(CAMwordlist_high_split, file = "CAMwordlist_high_split.xlsx")
# write.xlsx(CAMwordlist_low_split, file = "CAMwordlist_low_split.xlsx")

CAMwordlist_high_split$group <- "high"
CAMwordlist_low_split$group <- "low"



CAMwordlist_overall_split <- merge(CAMwordlist_high_split[c(1:7, length(CAMwordlist_high_split))], CAMwordlist_low_split[c(1:7, length(CAMwordlist_low_split))], by = "Words")

# raw_diff: difference in frequency of mentions
CAMwordlist_overall_split$raw_diff <- abs(CAMwordlist_overall_split$raw.x - CAMwordlist_overall_split$raw.y)

# mean_valence_diff: difference of mean valence
CAMwordlist_overall_split$mean_valence_diff <- abs(CAMwordlist_overall_split$mean_valence.x - CAMwordlist_overall_split$mean_valence.y)

CAMwordlist_overall_split

# write.xlsx(CAMwordlist_overall_split, file = "CAMwordlist_overall_split.xlsx")

```



### not split

#### Preparation high & low

```{r}
# Word list high
tmp_text <- str_remove_all(string = CAMfiles_high[[1]]$text,
                           pattern = "_positive$|_negative$|_neutral$|_ambivalent$")

CAMwordlist_high <- create_wordlist(
  dat_nodes = CAMfiles_high[[1]],
  dat_merged = CAMfiles_high[[3]],
  useSummarized = TRUE,
  order = "frequency",
  splitByValence = FALSE,
  comments = TRUE,
  raterSubsetWords = NULL,
  rater = FALSE
)

dat_nodes <- CAMfiles_high[[1]]
dat_nodes$text <-   dat_nodes$text_summarized
sum(stringr::str_detect(string = dat_nodes$text, pattern = "_positive$|_negative$|_neutral$|_ambivalent$")) < nrow(dat_nodes)


# Word list low
tmp_text <- str_remove_all(string = CAMfiles_low[[1]]$text,
                           pattern = "_positive$|_negative$|_neutral$|_ambivalent$")

CAMwordlist_low <- create_wordlist(
  dat_nodes = CAMfiles_low[[1]],
  dat_merged = CAMfiles_low[[3]],
  useSummarized = TRUE,
  order = "frequency",
  splitByValence = FALSE,
  comments = TRUE,
  raterSubsetWords = NULL,
  rater = FALSE
)

dat_nodes <- CAMfiles_low[[1]]
dat_nodes$text <-   dat_nodes$text_summarized
sum(stringr::str_detect(string = dat_nodes$text, pattern = "_positive$|_negative$|_neutral$|_ambivalent$")) < nrow(dat_nodes)

rm(tmp_text, dat_nodes)

```


#### Concept Comparisons

```{r}

# setwd("outputs/CAMwordlists")
# write.xlsx(CAMwordlist_high, file = "CAMwordlist_high.xlsx")
# write.xlsx(CAMwordlist_low, file = "CAMwordlist_low.xlsx")

CAMwordlist_high$group <- "high"
CAMwordlist_low$group <- "low"



CAMwordlist_overall <- merge(CAMwordlist_high[c(1:7, length(CAMwordlist_high))], CAMwordlist_low[c(1:7, length(CAMwordlist_low))], by = "Words")

# concepts "positive values" and "weather" exist only in CAMwordlist_high

# raw_diff: difference in frequency of mentions
CAMwordlist_overall$raw_diff <- abs(CAMwordlist_overall$raw.x - CAMwordlist_overall$raw.y)

# percent_diff: difference in percentage
CAMwordlist_overall$percent_diff <- abs(CAMwordlist_overall$percent.x - CAMwordlist_overall$percent.y)

# mean_valence_diff: difference of mean valence
CAMwordlist_overall$mean_valence_diff <- abs(CAMwordlist_overall$mean_valence.x - CAMwordlist_overall$mean_valence.y)

CAMwordlist_overall$raw.o <- rowSums(CAMwordlist_overall[, c("raw.x", "raw.y")])
CAMwordlist_overall$percent.o <- rowMeans(CAMwordlist_overall[, c("percent.x", "percent.y")])
CAMwordlist_overall$mean_valence.o <- rowMeans(CAMwordlist_overall[, c("mean_valence.x", "mean_valence.y")])

# write.xlsx(CAMwordlist_overall, file = "CAMwordlist_overall.xlsx")
```


##### Specific categories

Differences in main concept "climate change".

```{r}
nodes_high <- CAMfiles_high[[1]]
nodes_low <- CAMfiles_low[[1]]

nodes_high <- nodes_high[nodes_high$text == "Climate Change" | nodes_high$text == "climate change",]
nodes_low <- nodes_low[nodes_low$text == "Climate Change" | nodes_low$text == "climate change",]

nodes_high$group <- "high"
nodes_low$group <- "low"

nodes_overall <- rbind(nodes_high, nodes_low)
nodes_overall$value[nodes_overall$value == 10] <- 0

leveneTest(value ~ group, data = nodes_overall, center = "median")
t.test(nodes_overall$value ~ nodes_overall$group, var.equal = FALSE, alternative = "two.sided")
cohensD(nodes_overall$value ~ nodes_overall$group)

rm(nodes_high, nodes_low, nodes_overall)
```


Differences in any category. 

```{r}
category <- "conspirative narration"

nodes_high <- CAMfiles_high[[1]]
nodes_low <- CAMfiles_low[[1]]

# Remove valence split
nodes_high$text_summarized <- gsub("_positive|_negative|_neutral|_ambivalent", "", nodes_high$text_summarized)
nodes_low$text_summarized <- gsub("_positive|_negative|_neutral|_ambivalent", "", nodes_low$text_summarized)

# Filter data regarding the category
nodes_high <- nodes_high[nodes_high$text_summarized == category,]
nodes_low <- nodes_low[nodes_low$text_summarized == category,]

nodes_high$group <- "high"
nodes_low$group <- "low"

nodes_overall <- rbind(nodes_high, nodes_low)
nodes_overall$value[nodes_overall$value == 10] <- 0

nodes_overall$group <- as.factor(nodes_overall$group)

shapiro.test(nodes_overall$value)
leveneTest(value ~ group, data = nodes_overall, center = "median")

# Not normally distributed -> Mann-Whitney-U-Test/Wilcoxon-Test
wilcox_test(nodes_overall$value ~ nodes_overall$group,
            distribution = "exact")

# Effect size
z <-
  qnorm(as.numeric(pvalue(
    wilcox_test(nodes_overall$value ~ nodes_overall$group,
                distribution = "exact")
  )) / 2)
r <- z/sqrt(nrow(nodes_overall))
r

rm(category, nodes_high, nodes_low, z, r)
```

Wilcoxon-Test effect sizes:
|r| < 0.1 	no effect / very small effect
|r| = 0.1 	small effect
|r| = 0.3 	medium effect
|r| = 0.5 	large effect



```{r}
# correlation between number of mentions (%) and mean valence
plot(CAMwordlist_overall$percent_diff, CAMwordlist_overall$mean_valence_diff)
cor.test(CAMwordlist_overall$percent_diff, CAMwordlist_overall$mean_valence_diff, method = "pearson", alternative = "two.sided")
```


#.
# Leiden with pruning

Community detection based on the Leiden algorithm, similar to Louvain.
Repeatedly run with a host of different gamma and omega parameters.
The partitions are then pruned with ModularityPruning (http://github.com/ragibson/ModularityPruning) to keep only stable and modularity-optimal partitions.

Algo selbst: 
https://github.com/soelderer/livmats-basal-attributes-analysis/blob/julius/partII_CAMs/02_partII_CAMs.qmd 

Doku: 
https://modularitypruning.readthedocs.io/en/latest/index.html


## Preparation

```{python}
import numpy as np
import igraph as ig
from modularitypruning import prune_to_stable_partitions
from modularitypruning.leiden_utilities import repeated_parallel_leiden_from_gammas
from modularitypruning.champ_utilities import CHAMP_3D
from modularitypruning.parameter_estimation_utilities import domains_to_gamma_omega_estimates
from modularitypruning.plotting import plot_2d_domains_with_estimates
from modularitypruning.plotting import plot_estimates
from modularitypruning.champ_utilities import CHAMP_2D
from modularitypruning.parameter_estimation_utilities import ranges_to_gamma_estimates
from modularitypruning.plotting import plot_estimates
import matplotlib.pyplot as plt
import os
import python_helpers  # our local helper scripts
from python_helpers import print_partition

#import cairocffi

#import subprocess
#import sys
#
#subprocess.check_call([sys.executable, "-m", "pip", "install", "modularitypruning"])

```


## high

### split
split by valence

#### Create adjacency matrix
```{r}
sel_ids_high <- unique(CAMfiles_high[[1]]$participantCAM)
tmp_nodes <- CAMfiles_high[[1]]


CAMaggregated_high <-
  aggregate_CAMs(dat_merged = CAMfiles_high[[3]],
                 dat_nodes = tmp_nodes,
                 ids_CAMs = sel_ids_high)



adjacency_matrix_high_split <- CAMaggregated_high[[1]]

diag(adjacency_matrix_high_split) <- 0



# Cutoff: Use only concepts that are connected with other concepts at least 5 times. 
# rows_to_keep <- which(rowSums(adjacency_matrix_high_split) > 4)
rows_to_keep <- as.character(CAMwordlist_high_split$Words[CAMwordlist_high_split$raw > 4])
adjacency_matrix_high_split <- adjacency_matrix_high_split[rows_to_keep, rows_to_keep]
rm(rows_to_keep)



graph_object <- graph_from_adjacency_matrix(adjacency_matrix_high_split, mode = "undirected")
# graph_object <- simplify(graph = graph_object)
# graph_object
# plot(graph_object, vertex.color = "grey75")

# adjacency_matrix_high <- igraph::as_adjacency_matrix(graph_object) # not needed

# library("leiden")
# ?leiden

# save as .csv
setwd("outputs")
write.csv(adjacency_matrix_high_split, file = "adjacency_matrix_high_split.csv", row.names = FALSE)
setwd("..")

# for(i in seq(.1, 2, by = .1)){
#   print(i)
#   partition <- leiden::leiden(object = graph_object, resolution_parameter = i)
#   print(table(partition))
# }

rm(sel_ids_high)
```


#### Compute most robust gamma

```{python}
np.set_printoptions(threshold=np.inf)

with open("outputs/adjacency_matrix_high_split.csv", "r") as f:
    names = f.readline()
    
names = names.split(",")
names[0] = names[0][1:]     # strip the first "
names[-1] = names[-1][:-2]  # strip the last \n

names = dict(enumerate(names))

adj_matrix = np.loadtxt(open("outputs/adjacency_matrix_high_split.csv", "rb"), delimiter=",", skiprows=1)

print(adj_matrix)


# remove self-loops
np.fill_diagonal(adj_matrix, 0)

G = ig.Graph.Adjacency(matrix=adj_matrix, mode="undirected")

gamma_range = (0, 2)
leiden_gammas = np.linspace(*gamma_range, 10 ** 5)   # 100k runs of Leiden algorithm
# leiden_gammas = np.linspace(*gamma_range, 2000)
# TODO: how to set a seed??

partitions = repeated_parallel_leiden_from_gammas(G, leiden_gammas)

# TODO: how to set a seed??

# prune to the stable partitions from (gamma=0, omega=0) to (gamma=2, omega=2)
stable_parts = prune_to_stable_partitions(G, partitions, *gamma_range)

# run CHAMP to obtain the dominant partitions along with their regions of optimality
ranges = CHAMP_2D(G, partitions, gamma_0=0.0, gamma_f=2.0)

# append gamma estimate for each dominant partition onto the CHAMP domains
gamma_estimates = ranges_to_gamma_estimates(G, ranges)

# get some infos about our partitions
for stable_part in stable_parts:
    print_partition(stable_part, partitions, names, gamma_estimates)
    print("\n\n")


# # plot gamma estimates and domains of optimality
# plt.rc('text', usetex=True)
# plt.rc('font', family='serif')
# plot_estimates(gamma_estimates)
# plt.title(r"CHAMP Domains of Optimality and gamma Estimates", fontsize=14)
# plt.xlabel(r"gamma", fontsize=14)
# plt.ylabel("Number of communities", fontsize=14)
# plt.show()


```


#### Print partitions

```{r}
partition <- leiden::leiden(object = graph_object, resolution_parameter = 1.135706822267914) # enter gamma as resolution_parameter


for (i in sort(unique(partition))) {
  cat(
    "\nfor partion",
    i,
    "the following words have been found:\n",
    rownames(adjacency_matrix_high_split)[partition == i],
    "\n"
  )
}


library("RColorBrewer")
node.cols <- brewer.pal(max(c(3, partition)), "Pastel1")[partition]
# plot(graph_object, vertex.color = node.cols)


plot(
  graph_object,
  edge.arrow.size = 0,
  layout = layout_nicely,
  vertex.frame.color = "black",
  asp = .5,
  margin = -0.1,
  vertex.size = diag(CAMaggregated_high[[1]]) / max(diag(CAMaggregated_high[[1]])) *
    1,
  vertex.label.cex = .9,
  edge.weight = 2,
  vertex.color = node.cols
)
```

#### Save final partition solution

```{r}
#| label: save solution Leiden algorithm
h = 1
for (i in sort(unique(partition))) {
  if(h == 1){
    part_high_split <- data.frame(partition = i, words = rownames(adjacency_matrix_high_split)[partition == i])
  }else{
    part_high_split <- rbind(part_high_split, 
                     data.frame(partition = i, words = rownames(adjacency_matrix_high_split)[partition == i]))
  }
  
  h = h + 1
}



part_high_split$mean_valence <- NA
part_high_split$sd_valence <- NA
for(i in 1:nrow(part_high_split)){
  part_high_split$mean_valence[i] <- CAMwordlist_high_split$mean_valence[CAMwordlist_high_split$Words == part_high_split$words[i]]
  part_high_split$sd_valence[i] <- CAMwordlist_high_split$sd_valence[CAMwordlist_high_split$Words == part_high_split$words[i]]
}

part_high_split$words <- str_replace_all(string = part_high_split$words, pattern = " {2,6}", replacement = " ")
part_high_split

# setwd("outputs")
# write.xlsx(x = part_high_split, file = "part_high_split.xlsx")
# saveRDS(part_high_split, file = "part_high_split.rds")
# setwd("..")

```


### not split

#### Create adjacency matrix

```{r}
sel_ids_high <- unique(CAMfiles_high[[1]]$participantCAM)
tmp_nodes <- CAMfiles[[1]]
tmp_nodes$text_summarized <- str_remove_all(string = tmp_nodes$text_summarized,
                           pattern = "_positive$|_negative$|_neutral$|_ambivalent$")


CAMaggregated_high <-
  aggregate_CAMs(dat_merged = CAMfiles_high[[3]],
                 dat_nodes = tmp_nodes,
                 ids_CAMs = sel_ids_high)


CAMaggregated_high[[1]][1:5, 1:5]

## check for symmetry
all(colSums(CAMaggregated_high[[1]]) == rowSums(CAMaggregated_high[[1]]))
all(colSums(CAMaggregated_high[[5]][[1]]) == rowSums(CAMaggregated_high[[5]][[1]]))


adjacency_matrix_high <- CAMaggregated_high[[1]]

diag(adjacency_matrix_high) <- 0



# Cutoff: Use only concepts that are connected with other concepts at least 5 times. 
rows_to_keep <- as.character(CAMwordlist_high$Words[CAMwordlist_high$raw > 4])
adjacency_matrix_high <- adjacency_matrix_high[rows_to_keep, rows_to_keep]
rm(rows_to_keep)



graph_object <- graph_from_adjacency_matrix(adjacency_matrix_high, mode = "undirected")
# graph_object <- simplify(graph = graph_object)
# graph_object
# plot(graph_object, vertex.color = "grey75")

# adjacency_matrix_high <- igraph::as_adjacency_matrix(graph_object) # not needed

# library("leiden")
# ?leiden

# save as .csv
setwd("outputs")
write.csv(adjacency_matrix_high, file = "adjacency_matrix_high.csv", row.names = FALSE)
setwd("..")

# 
# for(i in seq(.1, 2, by = .1)){
#   print(i)
#   partition <- leiden::leiden(object = graph_object, resolution_parameter = i)
#   print(table(partition))
# }
```


#### Compute most robust gamma

```{python}
np.set_printoptions(threshold=np.inf)

with open("outputs/adjacency_matrix_high.csv", "r") as f:
    names = f.readline()
    
names = names.split(",")
names[0] = names[0][1:]     # strip the first "
names[-1] = names[-1][:-2]  # strip the last \n

names = dict(enumerate(names))

adj_matrix = np.loadtxt(open("outputs/adjacency_matrix_high.csv", "rb"), delimiter=",", skiprows=1)

print(adj_matrix)


# remove self-loops
np.fill_diagonal(adj_matrix, 0)

G = ig.Graph.Adjacency(matrix=adj_matrix, mode="undirected")

gamma_range = (0, 2)
leiden_gammas = np.linspace(*gamma_range, 10 ** 5)   # 100k runs of Leiden algorithm
# leiden_gammas = np.linspace(*gamma_range, 2000)
# TODO: how to set a seed??

partitions = repeated_parallel_leiden_from_gammas(G, leiden_gammas)

# TODO: how to set a seed??

# prune to the stable partitions from (gamma=0, omega=0) to (gamma=2, omega=2)
stable_parts = prune_to_stable_partitions(G, partitions, *gamma_range)

# run CHAMP to obtain the dominant partitions along with their regions of optimality
ranges = CHAMP_2D(G, partitions, gamma_0=0.0, gamma_f=2.0)

# append gamma estimate for each dominant partition onto the CHAMP domains
gamma_estimates = ranges_to_gamma_estimates(G, ranges)

# get some infos about our partitions
for stable_part in stable_parts:
    print_partition(stable_part, partitions, names, gamma_estimates)
    print("\n\n")



# # plot gamma estimates and domains of optimality
# plt.rc('text', usetex=True)
# plt.rc('font', family='serif')
# plot_estimates(gamma_estimates)
# plt.title(r"CHAMP Domains of Optimality and gamma Estimates", fontsize=14)
# plt.xlabel(r"gamma", fontsize=14)
# plt.ylabel("Number of communities", fontsize=14)
# plt.show()


```


#### Print partitions

```{r}
# partition <- leiden::leiden(object = graph_object, resolution_parameter = 1.27)
# partition <- leiden::leiden(object = graph_object, resolution_parameter = 1.37)
partition <- leiden::leiden(object = graph_object, resolution_parameter = 1.0501528004323497)

for (i in sort(unique(partition))) {
  cat(
    "\nfor partion",
    i,
    "the following words have been found:\n",
    rownames(adjacency_matrix_high)[partition == i],
    "\n"
  )
}


library("RColorBrewer")
node.cols <- brewer.pal(max(c(3, partition)), "Pastel1")[partition]
# plot(graph_object, vertex.color = node.cols)


plot(
  graph_object,
  edge.arrow.size = 0,
  layout = layout_nicely,
  vertex.frame.color = "black",
  asp = .5,
  margin = -0.1,
  vertex.size = diag(CAMaggregated_high[[1]]) / max(diag(CAMaggregated_high[[1]])) *
    20,
  vertex.label.cex = .9,
  edge.weight = 2,
  vertex.color = node.cols
)
```


#### Save final partition solution

```{r}
#| label: save solution Leiden algorithm
h = 1
for (i in sort(unique(partition))) {
  if(h == 1){
    part_high <- data.frame(partition = i, words = rownames(adjacency_matrix_high)[partition == i])
  }else{
    part_high <- rbind(part_high, 
                     data.frame(partition = i, words = rownames(adjacency_matrix_high)[partition == i]))
  }
  
  h = h + 1
}



part_high$mean_valence <- NA
part_high$sd_valence <- NA
for(i in 1:nrow(part_high)){
  part_high$mean_valence[i] <- CAMwordlist_high$mean_valence[CAMwordlist_high$Words == part_high$words[i]]
  part_high$sd_valence[i] <- CAMwordlist_high$sd_valence[CAMwordlist_high$Words == part_high$words[i]]
}

part_high$words <- str_replace_all(string = part_high$words, pattern = " {2,6}", replacement = " ")


# setwd("outputs")
# write.xlsx(x = part_high, file = "part_high.xlsx")
# saveRDS(part_high, file = "part_high.rds")
# setwd("..")

```


## low

### split

#### Create adjacency matrix

```{r}
sel_ids_low <- unique(CAMfiles_low[[1]]$participantCAM)
tmp_nodes <- CAMfiles_low[[1]]


CAMaggregated_low <-
  aggregate_CAMs(dat_merged = CAMfiles_low[[3]],
                 dat_nodes = tmp_nodes,
                 ids_CAMs = sel_ids_low)



adjacency_matrix_low_split <- CAMaggregated_low[[1]]

diag(adjacency_matrix_low_split) <- 0



# Cutoff: Use only concepts that are connected with other concepts at least 5 times. 
# rows_to_keep <- which(rowSums(adjacency_matrix_low_split) > 3)
rows_to_keep <- as.character(CAMwordlist_low_split$Words[CAMwordlist_low_split$raw > 4])
adjacency_matrix_low_split <- adjacency_matrix_low_split[rows_to_keep, rows_to_keep]
rm(rows_to_keep)



graph_object <-
  graph_from_adjacency_matrix(adjacency_matrix_low_split, mode = "undirected")
# graph_object <- simplify(graph = graph_object)
# graph_object
# plot(graph_object, vertex.color = "grey75")

# adjacency_matrix_low <- igraph::as_adjacency_matrix(graph_object) # not needed

# library("leiden")
# ?leiden

# save as .csv
setwd("outputs")
write.csv(adjacency_matrix_low_split, file = "adjacency_matrix_low_split.csv", row.names = FALSE)
setwd("..")

# for(i in seq(.1, 2, by = .1)){
#   print(i)
#   partition <- leiden::leiden(object = graph_object, resolution_parameter = i)
#   print(table(partition))
# }
```


#### Compute most robust gamma

```{python}
np.set_printoptions(threshold=np.inf)

with open("outputs/adjacency_matrix_low_split.csv", "r") as f:
    names = f.readline()
    
names = names.split(",")
names[0] = names[0][1:]     # strip the first "
names[-1] = names[-1][:-2]  # strip the last \n

names = dict(enumerate(names))

adj_matrix = np.loadtxt(open("outputs/adjacency_matrix_low_split.csv", "rb"), delimiter=",", skiprows=1)

print(adj_matrix)


# remove self-loops
np.fill_diagonal(adj_matrix, 0)

G = ig.Graph.Adjacency(matrix=adj_matrix, mode="undirected")

gamma_range = (0, 2)
# leiden_gammas = np.linspace(*gamma_range, 10 ** 5)   # 100k runs of Leiden algorithm
leiden_gammas = np.linspace(*gamma_range, 2000)
# TODO: how to set a seed??

partitions = repeated_parallel_leiden_from_gammas(G, leiden_gammas)

# TODO: how to set a seed??

# prune to the stable partitions from (gamma=0, omega=0) to (gamma=2, omega=2)
stable_parts = prune_to_stable_partitions(G, partitions, *gamma_range)

# run CHAMP to obtain the dominant partitions along with their regions of optimality
ranges = CHAMP_2D(G, partitions, gamma_0=0.0, gamma_f=2.0)

# append gamma estimate for each dominant partition onto the CHAMP domains
gamma_estimates = ranges_to_gamma_estimates(G, ranges)

# get some infos about our partitions
for stable_part in stable_parts:
    print_partition(stable_part, partitions, names, gamma_estimates)
    print("\n\n")



# # plot gamma estimates and domains of optimality
# plt.rc('text', usetex=True)
# plt.rc('font', family='serif')
# plot_estimates(gamma_estimates)
# plt.title(r"CHAMP Domains of Optimality and gamma Estimates", fontsize=14)
# plt.xlabel(r"gamma", fontsize=14)
# plt.ylabel("Number of communities", fontsize=14)
# plt.show()


```


#### Print partitions

```{r}
partition <- leiden::leiden(object = graph_object, resolution_parameter = 1.3295363184489188) # enter gamma as resolution_parameter

for (i in sort(unique(partition))) {
  cat(
    "\nfor partion",
    i,
    "the following words have been found:\n",
    rownames(adjacency_matrix_low_split)[partition == i],
    "\n"
  )
}




library("RColorBrewer")
node.cols <- brewer.pal(max(c(3, partition)), "Pastel1")[partition]
# plot(graph_object, vertex.color = node.cols)


plot(
  graph_object,
  edge.arrow.size = 0,
  layout = layout_nicely,
  vertex.frame.color = "black",
  asp = .5,
  margin = -0.1,
  vertex.size = diag(CAMaggregated_low[[1]]) / max(diag(CAMaggregated_low[[1]])) *
    1,
  vertex.label.cex = .9,
  edge.weight = 2,
  vertex.color = node.cols
)
```


#### Save final partition solution

```{r}
#| label: save solution Leiden algorithm
h = 1
for (i in sort(unique(partition))) {
  if(h == 1){
    part_low_split <- data.frame(partition = i, words = rownames(adjacency_matrix_low_split)[partition == i])
  }else{
    part_low_split <- rbind(part_low_split, 
                     data.frame(partition = i, words = rownames(adjacency_matrix_low_split)[partition == i]))
  }
  
  h = h + 1
}



part_low_split$mean_valence <- NA
part_low_split$sd_valence <- NA
for(i in 1:nrow(part_low_split)){
  part_low_split$mean_valence[i] <- CAMwordlist_low_split$mean_valence[CAMwordlist_low_split$Words == part_low_split$words[i]]
  part_low_split$sd_valence[i] <- CAMwordlist_low_split$sd_valence[CAMwordlist_low_split$Words == part_low_split$words[i]]
}

part_low_split$words <- str_replace_all(string = part_low_split$words, pattern = " {2,6}", replacement = " ")


# setwd("outputs")
# write.xlsx(x = part_low_split, file = "part_low_split.xlsx")
# saveRDS(part_low_split, file = "part_low_split.rds")
# setwd("..")

```



### not split

#### Create adjacency matrix

```{r}
sel_ids_low <- unique(CAMfiles_low[[1]]$participantCAM)
tmp_nodes <- CAMfiles[[1]]
tmp_nodes$text_summarized <- str_remove_all(string = tmp_nodes$text_summarized,
                           pattern = "_positive$|_negative$|_neutral$|_ambivalent$")


CAMaggregated_low <-
  aggregate_CAMs(dat_merged = CAMfiles_low[[3]],
                 dat_nodes = tmp_nodes,
                 ids_CAMs = sel_ids_low)


CAMaggregated_low[[1]][1:5, 1:5]

## check for symmetry
all(colSums(CAMaggregated_low[[1]]) == rowSums(CAMaggregated_low[[1]]))
all(colSums(CAMaggregated_low[[5]][[1]]) == rowSums(CAMaggregated_low[[5]][[1]]))


adjacency_matrix_low <- CAMaggregated_low[[1]]

diag(adjacency_matrix_low) <- 0



# Cutoff: Use only concepts that are connected with other concepts at least 5 times. 
# rows_to_keep <- which(rowSums(adjacency_matrix_low) > 3)
rows_to_keep <- as.character(CAMwordlist_low$Words[CAMwordlist_low$raw > 4])
adjacency_matrix_low <- adjacency_matrix_low[rows_to_keep, rows_to_keep]
rm(rows_to_keep)



graph_object <- graph_from_adjacency_matrix(adjacency_matrix_low, mode = "undirected")
# graph_object <- simplify(graph = graph_object)
# graph_object
# plot(graph_object, vertex.color = "grey75")

# adjacency_matrix_low <- igraph::as_adjacency_matrix(graph_object) # not needed

# library("leiden")
# ?leiden

# save as .csv
setwd("outputs")
write.csv(adjacency_matrix_low, file = "adjacency_matrix_low.csv", row.names = FALSE)
setwd("..")

# for(i in seq(.1, 2, by = .1)){
#   print(i)
#   partition <- leiden::leiden(object = graph_object, resolution_parameter = i)
#   print(table(partition))
# }
```


#### Compute most robust gamma

```{python}
np.set_printoptions(threshold=np.inf)

with open("outputs/adjacency_matrix_low.csv", "r") as f:
    names = f.readline()
    
names = names.split(",")
names[0] = names[0][1:]     # strip the first "
names[-1] = names[-1][:-2]  # strip the last \n

names = dict(enumerate(names))

adj_matrix = np.loadtxt(open("outputs/adjacency_matrix_low.csv", "rb"), delimiter=",", skiprows=1)

print(adj_matrix)


# remove self-loops
np.fill_diagonal(adj_matrix, 0)

G = ig.Graph.Adjacency(matrix=adj_matrix, mode="undirected")

gamma_range = (0, 2)
# leiden_gammas = np.linspace(*gamma_range, 10 ** 5)   # 100k runs of Leiden algorithm
leiden_gammas = np.linspace(*gamma_range, 2000)
# TODO: how to set a seed??

partitions = repeated_parallel_leiden_from_gammas(G, leiden_gammas)

# TODO: how to set a seed??

# prune to the stable partitions from (gamma=0, omega=0) to (gamma=2, omega=2)
stable_parts = prune_to_stable_partitions(G, partitions, *gamma_range)

# run CHAMP to obtain the dominant partitions along with their regions of optimality
ranges = CHAMP_2D(G, partitions, gamma_0=0.0, gamma_f=2.0)

# append gamma estimate for each dominant partition onto the CHAMP domains
gamma_estimates = ranges_to_gamma_estimates(G, ranges)

# get some infos about our partitions
for stable_part in stable_parts:
    print_partition(stable_part, partitions, names, gamma_estimates)
    print("\n\n")



# # plot gamma estimates and domains of optimality
# plt.rc('text', usetex=True)
# plt.rc('font', family='serif')
# plot_estimates(gamma_estimates)
# plt.title(r"CHAMP Domains of Optimality and gamma Estimates", fontsize=14)
# plt.xlabel(r"gamma", fontsize=14)
# plt.ylabel("Number of communities", fontsize=14)
# plt.show()


```


#### Print partitions

```{r}
partition <- leiden::leiden(object = graph_object, resolution_parameter = 0.9470849494275032) # enter gamma as resolution_parameter

for (i in sort(unique(partition))) {
  cat(
    "\nfor partion",
    i,
    "the following words have been found:\n",
    rownames(adjacency_matrix_low)[partition == i],
    "\n"
  )
}


library("RColorBrewer")
node.cols <- brewer.pal(max(c(3, partition)), "Pastel1")[partition]
# plot(graph_object, vertex.color = node.cols)


plot(
  graph_object,
  edge.arrow.size = 0,
  layout = layout_nicely,
  vertex.frame.color = "black",
  asp = .5,
  margin = -0.1,
  vertex.size = diag(CAMaggregated_low[[1]]) / max(diag(CAMaggregated_low[[1]])) *
    20,
  vertex.label.cex = .9,
  edge.weight = 2,
  vertex.color = node.cols
)
```


#### Save final partition solution

```{r}
#| label: save solution Leiden algorithm
h = 1
for (i in sort(unique(partition))) {
  if(h == 1){
    part_low <- data.frame(partition = i, words = rownames(adjacency_matrix_low)[partition == i])
  }else{
    part_low <- rbind(part_low, 
                     data.frame(partition = i, words = rownames(adjacency_matrix_low)[partition == i]))
  }
  
  h = h + 1
}



part_low$mean_valence <- NA
part_low$sd_valence <- NA
for(i in 1:nrow(part_low)){
  part_low$mean_valence[i] <- CAMwordlist_high_slowvalence[CAMwordlist_low$Words == part_low$words[i]]
  part_low$sd_valence[i] <- CAMwordlist_high_slowlence[CAMwordlist_low$Words == part_low$words[i]]
}

part_low$words <- str_replace_all(string = part_low$words, pattern = " {2,6}", replacement = " ")


# setwd("outputs")
# write.xlsx(x = part_low, file = "part_low.xlsx")
# saveRDS(part_low, file = "part_low.rds")
# setwd("..")

```


#.
# Analyse partition data

```{r}
sum_part_high_split <- part_high_split %>%
  group_by(partition) %>%
  summarise(
    Words = paste(words, collapse = ", "),
    M = mean(mean_valence),
    SD = mean(sd_valence)
  )

sum_part_low_split <- part_low_split %>%
  group_by(partition) %>%
  summarise(
    Words = paste(words, collapse = ", "),
    M = mean(mean_valence),
    SD = mean(sd_valence)
  )

# setwd("outputs/leiden_partitions")
# write.xlsx(sum_part_high_split, file = "sum_part_high_split.xlsx")
# write.xlsx(sum_part_low_split, file = "sum_part_low_split.xlsx")
# setwd("../..")

```



#.
# CliquePercolation Tests

## high

Tutorial: <https://cran.r-project.org/web/packages/CliquePercolation/vignettes/CliquePercolation.html>

Informative paper: \
Lange, J., (2021). CliquePercolation: An R Package for conducting and visualizing results of the clique percolation network community detection algorithm. Journal of Open Source Software, 6(62), 3210, <https://doi.org/10.21105/joss.03210>

-   *k*-cliques = fully connected networks with *k* nodes. The smallest possible k would be *k* = 3. Otherwise, the cliques would be only edges.
-   *I* = For weighted networks, the algorithm has just one intermediate additional step. Specifically, after identifying the k-cliques, they are considered further only if their Intensity exceeds a specified threshold I. The Intensity of a k-clique is defined as the geometric mean of the edge weights.

```{r cp}

library(CliquePercolation)

library(qgraph)

W <- adjacency_matrix_high # Some kind of matrix (adjacency or weight matrix that is symmetric)
W <- qgraph::qgraph(W, theme = "colorblind", layout = "spring", cut = 0.4) # Just for visualization 

### To run the clique percolation algorithm for weighted networks, we initially need to optimize k and I.

thresholds <- cpThreshold(W, method = "weighted", k.range = c(3,4),
                          I.range = c(seq(0.40, 0.01, by = -0.005)),
                          threshold = c("largest.components.ratio","chi")) # This function may need lots of time to run

##################### Supplementary Analysis: Permutation Test #####################

### Lange (2021): A permutation test, which repeatedly randomly shuffles the edges in the network and recalculates entropy can point out which entropy values are higher than already expected by chance.

threshold <- cpThreshold(W,
  method = "weighted",
  k.range = c(3, 4),
  I.range = seq(0.3, 0.09,-0.01),
  threshold = "entropy"
)

thresholds.permute <- cpPermuteEntropy(W, cpThreshold.object = threshold, seed = 4186)

# -> returning the combinations of k and I that are more surprising than chance

########################################## 

### Run the clique percolation method.

###### Additionally, we enter the optimal k and I values determined via cpThreshold. 

cpk3I.35 <- cpAlgorithm(W, k = 3, method = "weighted", I = 0.35)
cpk4I.27 <- cpAlgorithm(W, k = 4, method = "weighted", I = 0.27)

### Using summary, it is possible to get more detailed information about the communities, shared nodes, and isolated nodes.

summary(cpk3I.35)

### To look at the community size distribution.

cpCommunitySizeDistribution(cpk3I.35$list.of.communities.numbers)

### To actually test whether the distribution for k=3 fits a power-law (i.e., are k=3 and I=.35  the optimal parameters for clique percolation?)

fit_pl_k3I.35 <- cpCommunitySizeDistribution(cpk3I.35$list.of.communities.numbers, test.power.law = TRUE)

### Plotting the solution of the clique percolation algorithm in another network, such that a node represents a community, and the edges between nodes represent the number of nodes that two communities share (i.e., the community graph).

commnetwork <- cpCommunityGraph(cpk3I.35$list.of.communities.numbers,
                                node.size.method = "proportional",
                                max.node.size = 20,
                                theme = "colorblind", layout = "spring", repulsion = 0.4)

```

## low



#.
# analyze data 

## descriptives 

```{r}
plot(t12_questionnaireCAMs$total_min_prolific, t12_questionnaireCAMs$total_min_prolific_t2)
cor(t12_questionnaireCAMs$total_min_prolific, t12_questionnaireCAMs$total_min_prolific_t2, use = "pairwise")
```


## correlation plots

### to mean valence

```{r, fig.width=14}
psych::cor.plot(r = cor(t12_questionnaireCAMs[, str_detect(string = colnames(t12_questionnaireCAMs),
                                                   pattern = "^mean_")],
                                                   use = "pairwise.complete.obs"),
                                                   upper = FALSE, xlas = 2, main = "Germany")
```


### to number of concepts


```{r, fig.width=14}
psych::cor.plot(r = cor(t12_questionnaireCAMs[, str_detect(string = colnames(t12_questionnaireCAMs),
                                                   pattern = "^mean_[:alpha:]*$|^num_nodes")],
                                                   use = "pairwise.complete.obs"),
                                                   upper = FALSE, xlas = 2, main = "Germany")
```



### to latent parameters


```{r, fig.width=14}
psych::cor.plot(r = cor(questionnaire_CAM_Germany[, str_detect(string = colnames(questionnaire_CAM_Germany),
                                                   pattern = "^mean_[:alpha:]*$|^density|^transitivity.*macro$|^centr.*macro$|^meanDistance|^assortativity.*macro$")],
                                                   use = "pairwise.complete.obs"),
                                                   upper = FALSE, xlas = 2, main = "Germany")


psych::cor.plot(r = cor(questionnaire_CAM_USA[, str_detect(string = colnames(questionnaire_CAM_USA),
                                                   pattern = "^mean_[:alpha:]*$|^density|^transitivity.*macro$|^centr.*macro$|^meanDistance|^assortativity.*macro$")],
                                                   use = "pairwise.complete.obs"),
                                                   upper = FALSE, xlas = 2, main = "USA")
```


## mean differences

```{r}
# questionnaireCAMs$countryParty <- NA
# questionnaireCAMs$CAM_ID %in% questionnaire_t1$PROLIFIC_PID
# questionnaire_t1$politicalParty

ggbetweenstats(
  data = t12_questionnaireCAMs,
  x = classes_conspiracy,
  y = mean_valence_macro
)

ggbetweenstats(
  data = t12_questionnaireCAMs,
  x = classes_conspiracy,
  y = mean_CMQ
)

ggbetweenstats(
  data = t12_questionnaireCAMs,
  x = classes_conspiracy,
  y = mean_RiskPerception
)

ggbetweenstats(
  data = questionnaireCAMs,
  x = country,
  y = mean_BiosphericValues
)


ggbetweenstats(
  data = questionnaireCAMs,
  x = country,
  y = mean_PolicyItems
)

```

