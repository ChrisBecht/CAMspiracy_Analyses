---
title: "Analyze Merged Files from t1, t2"
author: "Julius Fenn, Christophe Becht"
format:
  html:
    toc: true
    toc-depth: 3
    html-math-method: katex
---


# Notes

```{r}
## global variables: 
```



# load merged pre-processed data

```{r}
#| echo: true
#| warning: false

# sets the directory of location of this script as the current directory
setwd(dirname(rstudioapi::getSourceEditorContext()$path))

# load packages
require(pacman)

p_load('tidyverse', 'jsonlite', 'magrittr', 'xlsx',
       'stargazer', 'psych', 'jtools', 'DT', 'ggstatsplot', 
       'lavaan', 'igraph',
       'regsem', 'MplusAutomation', 'reticulate')

library(openxlsx)
library(ggplot2)
library(RColorBrewer)

# reticulate::py_config()
# reticulate::py_module_available("pycairo")
# reticulate::py_module_available("cairocffi")


if(!reticulate::py_module_available("igraph")){
  py_install("igraph") # pip install python-igraph
}
if(!reticulate::py_module_available("modularitypruning")){
  py_install("modularitypruning") # pip install modularitypruning
}
if(!reticulate::py_module_available("matplotlib")){
  py_install("matplotlib")
}
if(!reticulate::py_module_available("scipy")){
  py_install("scipy")
}
if(!reticulate::py_module_available("leidenalg")){
  py_install("leidenalg")
}
if(!reticulate::py_module_available("numpy")){
  py_install("numpy")
}

## install Python modules
# py_install("matplotlib")
# py_install("scipy")
# py_install("cairocffi") 
# py_install("pycairo") # pip install pycairo

library(igraph)
library(leiden)
library(lsr)

# load data
setwd("outputs")
t12_questionnaireCAMs <- readRDS(file = "t12_questionnaireCAMs.rds")
CAMfiles <- readRDS(file = "CAMfiles.rds")
CAMdrawn <- readRDS(file = "CAMdrawn.rds")


t12_questionnaireCAMs$total_min_prolific[t12_questionnaireCAMs$total_min_prolific > 1000] <- NA


# load functions
setwd("../../functions")
for(i in 1:length(dir())){
  # print(dir()[i])
  source(dir()[i], encoding = "utf-8")
}


setwd("../functions_CAMapp")
for(i in 1:length(dir())){
  # print(dir()[i])
  source(dir()[i], encoding = "utf-8")
}
rm(i)
setwd("..")
```

```{r protocol}
setwd("data")
protocolDataset <- "protocol_fixed_15.txt"

consider_Protocol <- TRUE

if(consider_Protocol){
  text <- readLines(protocolDataset, warn = FALSE)
  text <- readLines(textConnection(text, encoding="UTF-8"), encoding="UTF-8")
  
  if (testIfJson(file = text)) {
    protocol <- rjson::fromJSON(file = protocolDataset)
  } else{
    print("Invalid protocol uploaded")
  }
}

if(consider_Protocol){
  CAMfiles[[1]] <- CAMfiles[[1]][CAMfiles[[1]]$CAM %in% protocol$currentCAMs,]
  CAMfiles[[2]] <- CAMfiles[[2]][CAMfiles[[2]]$CAM %in% protocol$currentCAMs,]
  CAMfiles[[3]] <- CAMfiles[[3]][CAMfiles[[3]]$CAM.x %in% protocol$currentCAMs,]


  tmp_out <- overwriteTextNodes(protocolDat = protocol,
                                nodesDat = CAMfiles[[1]])
  CAMfiles[[1]] <- tmp_out[[1]]
  tmp_out[[2]]
}

rm(protocolDataset, consider_Protocol, text, tmp_out)
```


# split data

Data set is split according to country (Germany & USA) and persons with high conspiracy (3) and low conspiracy (1). 

## Country | not necessary

### CAMs USA
```{r}
setwd("outputs")
if(!file.exists("CAMs_USA")){
  dir.create("CAMs_USA")
}
setwd("CAMs_USA")


CAMfiles_USA <- CAMfiles

## check is ID data set is complete
if(!all(CAMfiles_USA[[1]]$participantCAM %in% t12_questionnaireCAMs$PROLIFIC_PID)){
    print("Error")
}else{
  tmp_ids <- t12_questionnaireCAMs$PROLIFIC_PID[t12_questionnaireCAMs$country.x == "USA"]
  
  ## keep only CAM data from USA
  CAMfiles_USA[[1]] <- CAMfiles_USA[[1]][CAMfiles_USA[[1]]$participantCAM %in% tmp_ids,]
  CAMfiles_USA[[2]] <- CAMfiles_USA[[2]][CAMfiles_USA[[2]]$participantCAM %in% tmp_ids,]
  CAMfiles_USA[[3]] <- CAMfiles_USA[[3]][CAMfiles_USA[[3]]$participantCAM.x %in% tmp_ids,]
  
  ## save files ob subsets
  vroom::vroom_write(x =  CAMfiles_USA[[1]], file = "CAM_nodes_USA.txt")
  vroom::vroom_write(x =  CAMfiles_USA[[2]], file = "CAM_connectors_USA.txt")
  vroom::vroom_write(x =  CAMfiles_USA[[3]], file = "CAM_merged_USA.txt")
}
```


### CAMs Germany
```{r}
setwd("outputs")
if(!file.exists("CAMs_Germany")){
  dir.create("CAMs_Germany")
}
setwd("CAMs_Germany")


CAMfiles_Germany <- CAMfiles

## check is ID data set is complete
if(!all(CAMfiles_Germany[[1]]$participantCAM %in% t12_questionnaireCAMs$PROLIFIC_PID)){
    print("Error")
}else{
  tmp_ids <- t12_questionnaireCAMs$PROLIFIC_PID[t12_questionnaireCAMs$country.x == "Germany"]
  
  ## keep only CAM data from Germany
  CAMfiles_Germany[[1]] <- CAMfiles_Germany[[1]][CAMfiles_Germany[[1]]$participantCAM %in% tmp_ids,]
  CAMfiles_Germany[[2]] <- CAMfiles_Germany[[2]][CAMfiles_Germany[[2]]$participantCAM %in% tmp_ids,]
  CAMfiles_Germany[[3]] <- CAMfiles_Germany[[3]][CAMfiles_Germany[[3]]$participantCAM.x %in% tmp_ids,]
  
  ## save files ob subsets
  vroom::vroom_write(x =  CAMfiles_Germany[[1]], file = "CAM_nodes_Germany.txt")
  vroom::vroom_write(x =  CAMfiles_Germany[[2]], file = "CAM_connectors_Germany.txt")
  vroom::vroom_write(x =  CAMfiles_Germany[[3]], file = "CAM_merged_Germany.txt")
}
```


## classes_conspiracy: 3 = high, 1 = low conspiracy

### CAMs from high conspir belief

```{r}
setwd("outputs")
if(!file.exists("CAMs_high")){
  dir.create("CAMs_high")
}
setwd("CAMs_high")


CAMfiles_high <- CAMfiles

## check is ID data set is complete
if(!all(CAMfiles_high[[1]]$participantCAM %in% t12_questionnaireCAMs$PROLIFIC_PID)){
    print("Error")
}else{
  tmp_ids <- t12_questionnaireCAMs$PROLIFIC_PID[t12_questionnaireCAMs$classes_conspiracy == 3]
  
  ## keep only CAM data from high conspir
  CAMfiles_high[[1]] <- CAMfiles_high[[1]][CAMfiles_high[[1]]$participantCAM %in% tmp_ids,]
  CAMfiles_high[[2]] <- CAMfiles_high[[2]][CAMfiles_high[[2]]$participantCAM %in% tmp_ids,]
  CAMfiles_high[[3]] <- CAMfiles_high[[3]][CAMfiles_high[[3]]$participantCAM.x %in% tmp_ids,]
  
  ## save files ob subsets
  vroom::vroom_write(x =  CAMfiles_high[[1]], file = "CAM_nodes_high.txt")
  vroom::vroom_write(x =  CAMfiles_high[[2]], file = "CAM_connectors_high.txt")
  vroom::vroom_write(x =  CAMfiles_high[[3]], file = "CAM_merged_high.txt")
}

saveRDS(CAMfiles_high, file = "CAMfiles_high.RDS")
rm(tmp_ids)

setwd("..")
```


### CAMs from low conspir belief

```{r}
setwd("outputs")
if(!file.exists("CAMs_low")){
  dir.create("CAMs_low")
}
setwd("CAMs_low")


CAMfiles_low <- CAMfiles

## check is ID data set is complete
if(!all(CAMfiles_low[[1]]$participantCAM %in% t12_questionnaireCAMs$PROLIFIC_PID)){
    print("Error")
}else{
  tmp_ids <- t12_questionnaireCAMs$PROLIFIC_PID[t12_questionnaireCAMs$classes_conspiracy == 1]
  
  ## keep only CAM data from low conspir
  CAMfiles_low[[1]] <- CAMfiles_low[[1]][CAMfiles_low[[1]]$participantCAM %in% tmp_ids,]
  CAMfiles_low[[2]] <- CAMfiles_low[[2]][CAMfiles_low[[2]]$participantCAM %in% tmp_ids,]
  CAMfiles_low[[3]] <- CAMfiles_low[[3]][CAMfiles_low[[3]]$participantCAM.x %in% tmp_ids,]
  
  ## save files ob subsets
  vroom::vroom_write(x =  CAMfiles_low[[1]], file = "CAM_nodes_low.txt")
  vroom::vroom_write(x =  CAMfiles_low[[2]], file = "CAM_connectors_low.txt")
  vroom::vroom_write(x =  CAMfiles_low[[3]], file = "CAM_merged_low.txt")
}

saveRDS(CAMfiles_low, file = "CAMfiles_low.RDS")
rm(tmp_ids)

setwd("..")
```


#.
# analyze data

## Network Indicators

https://camtools-documentation.readthedocs.io/en/master/CAM-App/#compute-network-indicators

- num_nodes_macro: Number of concepts.
- num_nodes_pos_macro: Number of concepts.       
- num_nodes_neg_macro: Number of negative concepts.
- num_nodes_neut_macro: Number of neutral concepts.
- num_nodes_ambi_macro: Number of ambivalent concepts.
- mean_valence_macro: The mean valence of all concepts.
- density_macro: The density of a CAM refers to the ratio between the actual number of edges and the maximum possible number of edges in the CAM.
- num_edges_macro: Number of edges. 
- transitivity_macro: Transitivity measures the likelihood that the neighboring vertices of a vertex are connected. This is also known as the clustering coefficient.
- assortativity_valence_macro: The coefficient ranges from [-1, 1], whereby positive values indicate that concepts with the same valence have a tendency to connect with each other (level of homophily). The value -1 indicates a completely disassortative CAM.


### Preparation

```{r}
# high
CAMdrawn_high <- draw_CAM(dat_merged = CAMfiles_high[[3]],
                          dat_nodes = CAMfiles_high[[1]],
                          ids_CAMs = "all", plot_CAM = FALSE, useCoordinates = TRUE,
                          relvertexsize = 5,
                          reledgesize = 1)

netIndicators_high <- compute_indicatorsCAM(drawn_CAM = CAMdrawn_high)
netIndicators_high$group <- "high"



# low
CAMdrawn_low <- draw_CAM(dat_merged = CAMfiles_low[[3]],
                         dat_nodes = CAMfiles_low[[1]],
                         ids_CAMs = "all", plot_CAM = FALSE, useCoordinates = TRUE,
                         relvertexsize = 5,
                         reledgesize = 1)

netIndicators_low <- compute_indicatorsCAM(drawn_CAM = CAMdrawn_low)
netIndicators_low$group <- "low"


# overall
CAMdrawn_overall <- draw_CAM(dat_merged = CAMfiles[[3]],
                          dat_nodes = CAMfiles[[1]],
                          ids_CAMs = "all", plot_CAM = FALSE, useCoordinates = TRUE,
                          relvertexsize = 5,
                          reledgesize = 1)

netIndicators_overall <- rbind(netIndicators_high, netIndicators_low)

```


### Pie Charts

#### Overall
```{r}
data <- data.frame(
  pos = sum(netIndicators_overall$num_nodes_pos_macro),
  neg = sum(netIndicators_overall$num_nodes_neg_macro),
  neut = sum(netIndicators_overall$num_nodes_neut_macro),
  ambi = sum(netIndicators_overall$num_nodes_ambi_macro)
)

# Prozentwerte berechnen
percentages <- round(prop.table(as.matrix(data), margin = 1) * 100, 1)

# Daten in das richtige Format für ggplot2 bringen
df <- data.frame(
  variable = colnames(data),
  value = unlist(data),
  percent = unlist(percentages)
)

# Benennung der Variablen
variable_labels <- c("pos" = "positive", "neg" = "negative", "neut" = "neutral", "ambi" = "ambivalent")
df$variable <- factor(df$variable, levels = names(variable_labels))
df$variable_label <- variable_labels[df$variable]

# Kreisdiagramm erstellen
pie_overall <- ggplot(df, aes(x = "", y = value, fill = variable, label = paste0(percentages, "%"))) +
  geom_bar(stat = "identity", color = "black", width = 1) +
  coord_polar("y", start = 0) +
  geom_text(position = position_stack(vjust = 0.5), size = 4) +
  scale_fill_manual(values = c("pos" = "green", "neg" = "red", "neut" = "yellow", "ambi" = "purple"), name = "Concept type:", labels = variable_labels) +
  theme_void() + 
  theme(
    legend.position = "right" # Legende auf der rechten Seite anzeigen
  )

# Kreisdiagramm anzeigen
print(pie_overall)

rm(df, percentages, data, pie_overall)
```


#### high
```{r}
data <- data.frame(
  pos = sum(netIndicators_high$num_nodes_pos_macro),
  neg = sum(netIndicators_high$num_nodes_neg_macro),
  neut = sum(netIndicators_high$num_nodes_neut_macro),
  ambi = sum(netIndicators_high$num_nodes_ambi_macro)
)

# Prozentwerte berechnen
percentages <- round(prop.table(as.matrix(data), margin = 1) * 100, 1)

# Daten in das richtige Format für ggplot2 bringen
df <- data.frame(
  variable = colnames(data),
  value = unlist(data),
  percent = unlist(percentages)
)

# Benennung der Variablen
variable_labels <- c("pos" = "positive", "neg" = "negative", "neut" = "neutral", "ambi" = "ambivalent")
df$variable <- factor(df$variable, levels = names(variable_labels))
df$variable_label <- variable_labels[df$variable]

# Kreisdiagramm erstellen
pie_high <- ggplot(df, aes(x = "", y = value, fill = variable, label = paste0(percentages, "%"))) +
  geom_bar(stat = "identity", color = "black", width = 1) +
  coord_polar("y", start = 0) +
  geom_text(position = position_stack(vjust = 0.5), size = 4) +
  scale_fill_manual(values = c("pos" = "green", "neg" = "red", "neut" = "yellow", "ambi" = "purple"), name = "Concept type:", labels = variable_labels) +
  theme_void() +
  theme(
    legend.position = "right" # Legende auf der rechten Seite anzeigen
  )

# Kreisdiagramm anzeigen
print(pie_high)

rm(df, percentages, data, pie_high)
```


#### low
```{r}
data <- data.frame(
  pos = sum(netIndicators_low$num_nodes_pos_macro),
  neg = sum(netIndicators_low$num_nodes_neg_macro),
  neut = sum(netIndicators_low$num_nodes_neut_macro),
  ambi = sum(netIndicators_low$num_nodes_ambi_macro)
)

# Prozentwerte berechnen
percentages <- round(prop.table(as.matrix(data), margin = 1) * 100, 1)

# Daten in das richtige Format für ggplot2 bringen
df <- data.frame(
  variable = colnames(data),
  value = unlist(data),
  percent = unlist(percentages)
)

# Benennung der Variablen
variable_labels <- c("pos" = "positive", "neg" = "negative", "neut" = "neutral", "ambi" = "ambivalent")
df$variable <- factor(df$variable, levels = names(variable_labels))
df$variable_label <- variable_labels[df$variable]

# Kreisdiagramm erstellen
pie_low <- ggplot(df, aes(x = "", y = value, fill = variable, label = paste0(percentages, "%"))) +
  geom_bar(stat = "identity", color = "black", width = 1) +
  coord_polar("y", start = 0) +
  geom_text(position = position_stack(vjust = 0.5), size = 4) +
  scale_fill_manual(values = c("pos" = "green", "neg" = "red", "neut" = "yellow", "ambi" = "purple"), name = "Concept type:", labels = variable_labels) +
  theme_void() +
  theme(
    legend.position = "right" # Legende auf der rechten Seite anzeigen
  )

# Kreisdiagramm anzeigen
print(pie_low)

rm(df, percentages, data, pie_low)
```


### t-Tests prerequisites

```{r}
library(car)

# number of concepts
leveneTest(num_nodes_macro ~ group, data = netIndicators_overall, center = "median")


# number of positive concepts
leveneTest(num_nodes_pos_macro ~ group, data = netIndicators_overall, center = "median")


# number of negative concepts
leveneTest(num_nodes_neg_macro ~ group, data = netIndicators_overall, center = "median")


# number of neutral concepts
leveneTest(num_nodes_neut_macro ~ group, data = netIndicators_overall, center = "median")


# number of ambivalent concepts
leveneTest(num_nodes_ambi_macro ~ group, data = netIndicators_overall, center = "median")


# mean valence
leveneTest(mean_valence_macro ~ group, data = netIndicators_overall, center = "median")


# number of edges
leveneTest(num_edges_macro ~ group, data = netIndicators_overall, center = "median")


# density
leveneTest(density_macro ~ group, data = netIndicators_overall, center = "median")


# transitivity
leveneTest(transitivity_macro ~ group, data = netIndicators_overall, center = "median")


# assortativity
leveneTest(assortativity_valence_macro ~ group, data = netIndicators_overall, center = "median")

```


### Table of Network Indicators

```{r}
netIndicators_table <- data.frame(
  indicator = c(
    "n",
    "number of concepts",
    "positive concepts",
    "negative concepts",
    "neutral concepts",
    "ambivalent concepts",
    "mean valence",
    "number of edges",
    "density",
    "transitivity",
    "assortativity valence"
  ),
  
  overall = c(
    nrow(netIndicators_overall),
    round(mean(netIndicators_overall$num_nodes_macro), 2),
    round(mean(netIndicators_overall$num_nodes_pos_macro), 2),
    round(mean(netIndicators_overall$num_nodes_neg_macro), 2),
    round(mean(netIndicators_overall$num_nodes_neut_macro), 2),
    round(mean(netIndicators_overall$num_nodes_ambi_macro), 2),
    round(mean(netIndicators_overall$mean_valence_macro), 2),
    round(mean(netIndicators_overall$num_edges_macro), 2),
    round(mean(netIndicators_overall$density_macro), 2),
    round(mean(netIndicators_overall$transitivity_macro), 2),
    round(mean(netIndicators_overall$assortativity_valence_macro, na.rm = TRUE), 2)
  ),
  
  SD_o = c(
    "-",
    round(sd(netIndicators_overall$num_nodes_macro), 2),
    round(sd(netIndicators_overall$num_nodes_pos_macro), 2),
    round(sd(netIndicators_overall$num_nodes_neg_macro), 2),
    round(sd(netIndicators_overall$num_nodes_neut_macro), 2),
    round(sd(netIndicators_overall$num_nodes_ambi_macro), 2),
    round(sd(netIndicators_overall$mean_valence_macro), 2),
    round(sd(netIndicators_overall$num_edges_macro), 2),
    round(sd(netIndicators_overall$density_macro), 2),
    round(sd(netIndicators_overall$transitivity_macro), 2),
    round(sd(netIndicators_overall$assortativity_valence_macro, na.rm = TRUE), 2)
  ),
  
  high = c(
    nrow(netIndicators_high),
    round(mean(netIndicators_high$num_nodes_macro), 2),
    round(mean(netIndicators_high$num_nodes_pos_macro), 2),
    round(mean(netIndicators_high$num_nodes_neg_macro), 2),
    round(mean(netIndicators_high$num_nodes_neut_macro), 2),
    round(mean(netIndicators_high$num_nodes_ambi_macro), 2),
    round(mean(netIndicators_high$mean_valence_macro), 2),
    round(mean(netIndicators_high$num_edges_macro), 2),
    round(mean(netIndicators_high$density_macro), 2),
    round(mean(netIndicators_high$transitivity_macro), 2),
    round(mean(netIndicators_high$assortativity_valence_macro, na.rm = TRUE), 2)
  ),
  
  SD_h = c(
    "-",
    round(sd(netIndicators_high$num_nodes_macro), 2),
    round(sd(netIndicators_high$num_nodes_pos_macro), 2),
    round(sd(netIndicators_high$num_nodes_neg_macro), 2),
    round(sd(netIndicators_high$num_nodes_neut_macro), 2),
    round(sd(netIndicators_high$num_nodes_ambi_macro), 2),
    round(sd(netIndicators_high$mean_valence_macro), 2),
    round(sd(netIndicators_high$num_edges_macro), 2),
    round(sd(netIndicators_high$density_macro), 2),
    round(sd(netIndicators_high$transitivity_macro), 2),
    round(sd(netIndicators_high$assortativity_valence_macro, na.rm = TRUE), 2)
  ),
  
  low = c(
    nrow(netIndicators_low),
    round(mean(netIndicators_low$num_nodes_macro), 2),
    round(mean(netIndicators_low$num_nodes_pos_macro), 2),
    round(mean(netIndicators_low$num_nodes_neg_macro), 2),
    round(mean(netIndicators_low$num_nodes_neut_macro), 2),
    round(mean(netIndicators_low$num_nodes_ambi_macro), 2),
    round(mean(netIndicators_low$mean_valence_macro), 2),
    round(mean(netIndicators_low$num_edges_macro), 2),
    round(mean(netIndicators_low$density_macro), 2),
    round(mean(netIndicators_low$transitivity_macro), 2),
    round(mean(netIndicators_low$assortativity_valence_macro, na.rm = TRUE), 2)
  ),
  
  SD_l = c(
    "-",
    round(sd(netIndicators_low$num_nodes_macro), 2),
    round(sd(netIndicators_low$num_nodes_pos_macro), 2),
    round(sd(netIndicators_low$num_nodes_neg_macro), 2),
    round(sd(netIndicators_low$num_nodes_neut_macro), 2),
    round(sd(netIndicators_low$num_nodes_ambi_macro), 2),
    round(sd(netIndicators_low$mean_valence_macro), 2),
    round(sd(netIndicators_low$num_edges_macro), 2),
    round(sd(netIndicators_low$density_macro), 2),
    round(sd(netIndicators_low$transitivity_macro), 2),
    round(sd(netIndicators_low$assortativity_valence_macro, na.rm = TRUE), 2)
  ),
  
  t.119 = c(
    "-",
    round(t.test(
        netIndicators_high$num_nodes_macro,
        netIndicators_low$num_nodes_macro,
        var.equal = TRUE, alternative = "two.sided"
      )$statistic, 2),
    round(t.test(
        netIndicators_high$num_nodes_pos_macro,
        netIndicators_low$num_nodes_pos_macro,
        var.equal = TRUE, alternative = "two.sided"
      )$statistic, 2),
    round(t.test(
        netIndicators_high$num_nodes_neg_macro,
        netIndicators_low$num_nodes_neg_macro,
        var.equal = TRUE, alternative = "two.sided"
      )$statistic, 2),
    round(t.test(
        netIndicators_high$num_nodes_neut_macro,
        netIndicators_low$num_nodes_neut_macro,
        var.equal = TRUE,
        alternative = "two.sided"
      )$statistic, 2),
    round(t.test(
        netIndicators_high$num_nodes_ambi_macro,
        netIndicators_low$num_nodes_ambi_macro,
        var.equal = TRUE, alternative = "two.sided"
      )$statistic, 2),
    round(t.test(
        netIndicators_high$mean_valence_macro,
        netIndicators_low$mean_valence_macro,
        var.equal = TRUE, alternative = "two.sided"
      )$statistic, 2),
    round(t.test(
        netIndicators_high$num_edges_macro,
        netIndicators_low$num_edges_macro,
        var.equal = TRUE, alternative = "two.sided"
      )$statistic, 2),
    round(t.test(
        netIndicators_high$density_macro,
        netIndicators_low$density_macro,
        var.equal = TRUE, alternative = "two.sided"
      )$statistic, 2),
    round(t.test(
        netIndicators_high$transitivity_macro,
        netIndicators_low$transitivity_macro,
        var.equal = TRUE, alternative = "two.sided"
      )$statistic, 2),
    round(t.test(
        netIndicators_high$assortativity_valence_macro,
        netIndicators_low$assortativity_valence_macro,
        var.equal = TRUE, alternative = "two.sided"
      )$statistic, 2)
  ),
  
  p = c(
    "-",
    round(t.test(
        netIndicators_high$num_nodes_macro,
        netIndicators_low$num_nodes_macro,
        var.equal = TRUE, alternative = "two.sided"
      )$p.value, 3),
    round(t.test(
        netIndicators_high$num_nodes_pos_macro,
        netIndicators_low$num_nodes_pos_macro,
        var.equal = TRUE, alternative = "two.sided"
      )$p.value, 3),
    round(t.test(
        netIndicators_high$num_nodes_neg_macro,
        netIndicators_low$num_nodes_neg_macro,
        var.equal = TRUE, alternative = "two.sided"
      )$p.value, 3),
    round(t.test(
        netIndicators_high$num_nodes_neut_macro,
        netIndicators_low$num_nodes_neut_macro,
        var.equal = TRUE,
        alternative = "two.sided"
      )$p.value, 3),
    round(t.test(
        netIndicators_high$num_nodes_ambi_macro,
        netIndicators_low$num_nodes_ambi_macro,
        var.equal = TRUE, alternative = "two.sided"
      )$p.value, 3),
    round(t.test(
        netIndicators_high$mean_valence_macro,
        netIndicators_low$mean_valence_macro,
        var.equal = TRUE, alternative = "two.sided"
      )$p.value, 3),
    round(t.test(
        netIndicators_high$num_nodes_macro,
        netIndicators_low$num_nodes_macro,
        var.equal = TRUE, alternative = "two.sided"
      )$p.value, 3),
    round(t.test(
        netIndicators_high$density_macro,
        netIndicators_low$density_macro,
        var.equal = TRUE, alternative = "two.sided"
      )$p.value, 3),
    round(t.test(
        netIndicators_high$transitivity_macro,
        netIndicators_low$transitivity_macro,
        var.equal = TRUE, alternative = "two.sided"
      )$p.value, 3),
    round(t.test(
        netIndicators_high$assortativity_valence_macro,
        netIndicators_low$assortativity_valence_macro,
        var.equal = TRUE, alternative = "two.sided"
      )$p.value, 3)
  ),
  Cohen_d = c(
    "-",
    "-",
    "-",
    round(cohensD(netIndicators_high$num_nodes_neg_macro,
                  netIndicators_low$num_nodes_neg_macro), 2),
    "-",
    "-",
    round(cohensD(netIndicators_high$mean_valence_macro,
                  netIndicators_low$mean_valence_macro), 2),
    "-",
    "-",
    "-",
    "-"
  )
)

netIndicators_table

```


## Concepts (Word lists)

### Split by valence

#### Preparation

```{r}
# Word list high
tmp_text <- str_remove_all(string = CAMfiles_high[[1]]$text,
                           pattern = "_positive$|_negative$|_neutral$|_ambivalent$")

CAMwordlist_high_split <- create_wordlist(
  dat_nodes = CAMfiles_high[[1]],
  dat_merged = CAMfiles_high[[3]],
  useSummarized = TRUE,
  order = "frequency",
  splitByValence = TRUE,
  comments = TRUE,
  raterSubsetWords = NULL,
  rater = FALSE
)

dat_nodes <- CAMfiles_high[[1]]
dat_nodes$text <-   dat_nodes$text_summarized
sum(stringr::str_detect(string = dat_nodes$text, pattern = "_positive$|_negative$|_neutral$|_ambivalent$")) < nrow(dat_nodes)


# Word list low
tmp_text <- str_remove_all(string = CAMfiles_low[[1]]$text,
                           pattern = "_positive$|_negative$|_neutral$|_ambivalent$")

CAMwordlist_low_split <- create_wordlist(
  dat_nodes = CAMfiles_low[[1]],
  dat_merged = CAMfiles_low[[3]],
  useSummarized = TRUE,
  order = "frequency",
  splitByValence = TRUE,
  comments = TRUE,
  raterSubsetWords = NULL,
  rater = FALSE
)

dat_nodes <- CAMfiles_low[[1]]
dat_nodes$text <-   dat_nodes$text_summarized
sum(stringr::str_detect(string = dat_nodes$text, pattern = "_positive$|_negative$|_neutral$|_ambivalent$")) < nrow(dat_nodes)

rm(tmp_text)

```


#### Concept Comparisons

```{r}

setwd("outputs/CAMwordlists")
write.xlsx(CAMwordlist_high_split, file = "CAMwordlist_high_split.xlsx")
write.xlsx(CAMwordlist_low_split, file = "CAMwordlist_low_split.xlsx")

CAMwordlist_high_split$group <- "high"
CAMwordlist_low_split$group <- "low"



CAMwordlist_overall_split <- merge(CAMwordlist_high_split[c(1:7, length(CAMwordlist_high_split))], CAMwordlist_low_split[c(1:7, length(CAMwordlist_low_split))], by = "Words")

# raw_diff: difference in frequency of mentions
CAMwordlist_overall_split$raw_diff <- abs(CAMwordlist_overall_split$raw.x - CAMwordlist_overall_split$raw.y)

# mean_valence_diff: difference of mean valence
CAMwordlist_overall_split$mean_valence_diff <- abs(CAMwordlist_overall_split$mean_valence.x - CAMwordlist_overall_split$mean_valence.y)

CAMwordlist_overall_split

write.xlsx(CAMwordlist_overall_split, file = "CAMwordlist_overall_split.xlsx")

```


### Not split by valence

#### Preparation

```{r}
# Word list high
tmp_text <- str_remove_all(string = CAMfiles_high[[1]]$text,
                           pattern = "_positive$|_negative$|_neutral$|_ambivalent$")

CAMwordlist_high <- create_wordlist(
  dat_nodes = CAMfiles_high[[1]],
  dat_merged = CAMfiles_high[[3]],
  useSummarized = TRUE,
  order = "frequency",
  splitByValence = FALSE,
  comments = TRUE,
  raterSubsetWords = NULL,
  rater = FALSE
)

dat_nodes <- CAMfiles_high[[1]]
dat_nodes$text <-   dat_nodes$text_summarized
sum(stringr::str_detect(string = dat_nodes$text, pattern = "_positive$|_negative$|_neutral$|_ambivalent$")) < nrow(dat_nodes)


# Word list low
tmp_text <- str_remove_all(string = CAMfiles_low[[1]]$text,
                           pattern = "_positive$|_negative$|_neutral$|_ambivalent$")

CAMwordlist_low <- create_wordlist(
  dat_nodes = CAMfiles_low[[1]],
  dat_merged = CAMfiles_low[[3]],
  useSummarized = TRUE,
  order = "frequency",
  splitByValence = FALSE,
  comments = TRUE,
  raterSubsetWords = NULL,
  rater = FALSE
)

dat_nodes <- CAMfiles_low[[1]]
dat_nodes$text <-   dat_nodes$text_summarized
sum(stringr::str_detect(string = dat_nodes$text, pattern = "_positive$|_negative$|_neutral$|_ambivalent$")) < nrow(dat_nodes)

rm(tmp_text)

```


#### Concept Comparisons

```{r}

setwd("outputs/CAMwordlists")
write.xlsx(CAMwordlist_high, file = "CAMwordlist_high.xlsx")
write.xlsx(CAMwordlist_low, file = "CAMwordlist_low.xlsx")

CAMwordlist_high$group <- "high"
CAMwordlist_low$group <- "low"



CAMwordlist_overall <- merge(CAMwordlist_high[c(1:7, length(CAMwordlist_high))], CAMwordlist_low[c(1:7, length(CAMwordlist_low))], by = "Words")

# raw_diff: difference in frequency of mentions
CAMwordlist_overall$raw_diff <- abs(CAMwordlist_overall$raw.x - CAMwordlist_overall$raw.y)

# mean_valence_diff: difference of mean valence
CAMwordlist_overall$mean_valence_diff <- abs(CAMwordlist_overall$mean_valence.x - CAMwordlist_overall$mean_valence.y)

write.xlsx(CAMwordlist_overall, file = "CAMwordlist_overall.xlsx")

```


## Leiden Algorithm

##################
### high conspir
##################


#### aggregate CAMs

```{r}
sel_ids_high <- unique(CAMfiles_high[[1]]$participantCAM)
CAMaggregated_high <-
  aggregate_CAMs(dat_merged = CAMfiles[[3]],
                 dat_nodes = CAMfiles[[1]],
                 ids_CAMs = sel_ids_high)


## check for symmetry
all(colSums(CAMaggregated_high[[5]][[1]]) == rowSums(CAMaggregated_high[[5]][[1]]))


## test first concept of first CAM
colnames(CAMaggregated_high[[5]][[1]])[1]
colnames(CAMaggregated_high[[5]][[1]])[CAMaggregated_high[[5]][[1]][1, ] >= 1]
rownames(CAMaggregated_high[[5]][[1]])[CAMaggregated_high[[5]][[1]][1, ] >= 1]

# plot(
#   make_ego_graph( CAMdrawn[[1]], order = 1, 1)[[1]],
#   edge.arrow.size = .7,
#   layout = layout_nicely,
#   vertex.frame.color = "black",
#   asp = .5,
#   margin = -0.1,
#   vertex.size = 10,
#   vertex.label.cex = .9,
#   main = networkIndicators$CAM_ID[i]
# )



## adjacency matrix (Aij = 1 when nodes i and j are connected and Aij = 0 otherwise)
CAMaggregated_onlyOnes_high <- CAMaggregated_high[[5]]
for(i in 1:length(CAMaggregated_onlyOnes_high)){
  CAMaggregated_onlyOnes_high[[i]][CAMaggregated_onlyOnes_high[[i]] >= 1] <- 1
}

rm(sel_ids_high, i)
```


#### Leiden algorithm

```{r}
adjacency_matrix_high <- CAMaggregated_high[[1]]

graph_object_high <-
  graph_from_adjacency_matrix(adjacency_matrix_high, mode = "directed")
# graph_object_high
# plot(graph_object_high, vertex.color = "grey75")

adjacency_matrix_high <- igraph::as_adjacency_matrix(graph_object_high) # not needed

partition_high <- leiden(adjacency_matrix_high)
table(partition_high)


for (i in unique(partition_high)) {
  cat(
    "\nfor partion_high",
    i,
    "the following words have been found:\n",
    rownames(adjacency_matrix_high)[partition_high == i],
    "\n"
  )
}
rm(i)

node.cols_high <- brewer.pal(max(c(3, partition_high)), "Pastel1")[partition_high]
# plot(graph_object, vertex.color = node.cols)


plot(
  graph_object_high,
  edge.arrow.size = 0,
  layout = layout_nicely,
  vertex.frame.color = "black",
  asp = .5,
  margin = -0.1,
  vertex.size = diag(CAMaggregated_high[[1]]) / max(diag(CAMaggregated_high[[1]])) *
    5,
  vertex.label.cex = .9,
  edge.weight = 2,
  vertex.color = node.cols_high
)

rm(node.cols_high)

```



##################
### low conspir
##################

#### aggregate CAMs

```{r}
sel_ids_low <- unique(CAMfiles_low[[1]]$participantCAM)
CAMaggregated_low <-
  aggregate_CAMs(dat_merged = CAMfiles[[3]],
                 dat_nodes = CAMfiles[[1]],
                 ids_CAMs = sel_ids_low)


## check for symmetry
all(colSums(CAMaggregated_low[[5]][[1]]) == rowSums(CAMaggregated_low[[5]][[1]]))


## test first concept of first CAM
colnames(CAMaggregated_low[[5]][[1]])[1]
colnames(CAMaggregated_low[[5]][[1]])[CAMaggregated_low[[5]][[1]][1, ] >= 1]
rownames(CAMaggregated_low[[5]][[1]])[CAMaggregated_low[[5]][[1]][1, ] >= 1]

# plot(
#   make_ego_graph( CAMdrawn[[1]], order = 1, 1)[[1]],
#   edge.arrow.size = .7,
#   layout = layout_nicely,
#   vertex.frame.color = "black",
#   asp = .5,
#   margin = -0.1,
#   vertex.size = 10,
#   vertex.label.cex = .9,
#   main = networkIndicators$CAM_ID[i]
# )



## adjacency matrix (Aij = 1 when nodes i and j are connected and Aij = 0 otherwise)
CAMaggregated_onlyOnes_low <- CAMaggregated_low[[5]]
for(i in 1:length(CAMaggregated_onlyOnes_low)){
  CAMaggregated_onlyOnes_low[[i]][CAMaggregated_onlyOnes_low[[i]] >= 1] <- 1
}

rm(sel_ids_low, i)

```


#### Leiden algorithm

```{r}
adjacency_matrix_low <- CAMaggregated_low[[1]]

graph_object_low <-
  graph_from_adjacency_matrix(adjacency_matrix_low, mode = "directed")
# graph_object_low
# plot(graph_object_low, vertex.color = "grey75")

adjacency_matrix_low <- igraph::as_adjacency_matrix(graph_object_low) # not needed

partition_low <- leiden(adjacency_matrix_low)
table(partition_low)


for (i in unique(partition_low)) {
  cat(
    "\nfor partion_low",
    i,
    "the following words have been found:\n",
    rownames(adjacency_matrix_low)[partition_low == i],
    "\n"
  )
}
rm(i)

node.cols_low <- brewer.pal(max(c(3, partition_low)), "Pastel1")[partition_low]
# plot(graph_object, vertex.color = node.cols)


plot(
  graph_object_low,
  edge.arrow.size = 0,
  layout = layout_nicely,
  vertex.frame.color = "black",
  asp = .5,
  margin = -0.1,
  vertex.size = diag(CAMaggregated_low[[1]]) / max(diag(CAMaggregated_low[[1]])) *
    5,
  vertex.label.cex = .9,
  edge.weight = 2,
  vertex.color = node.cols_low
)

rm(node.cols_low)
```


#.
# TEST: Leiden algorithm with pruning (Gibson and Mucha, 2022)

Community detection based on the Leiden algorithm, similar to Louvain.
Repeatedly run with a host of different gamma and omega parameters.
The partitions are then pruned with ModularityPruning (http://github.com/ragibson/ModularityPruning) to keep only stable and modularity-optimal partitions.

Konzepte durch Kategorien ersetzen
mit aggregierten CAMs vs. mit individuellen

Datenaufbereitung: 
Algo selbst: 
https://github.com/soelderer/livmats-basal-attributes-analysis/blob/julius/partII_CAMs/02_partII_CAMs.qmd  ab Z. ca. 200 - 360

Doku: 
https://modularitypruning.readthedocs.io/en/latest/index.html


```{python Algo}
import numpy as np
import igraph as ig
from modularitypruning import prune_to_multilayer_stable_partitions
from modularitypruning.leiden_utilities import repeated_parallel_leiden_from_gammas_omegas
from modularitypruning.champ_utilities import CHAMP_3D
from modularitypruning.parameter_estimation_utilities import domains_to_gamma_omega_estimates
from modularitypruning.plotting import plot_2d_domains_with_estimates
import matplotlib.pyplot as plt

import scipy.io
import os

#import subprocess
#import sys
#
#subprocess.check_call([sys.executable, "-m", "pip", "install", "modularitypruning"])

np.set_printoptions(threshold=np.inf)

CAMaggregated = scipy.io.loadmat("partII_CAMs/outputs/01_dataPreperation/final/CAMaggregated_adj_matrices.mat")
adj_matrices = list(CAMaggregated["multigraph_adj_matrices_list"][0,0])

num_layers = len(adj_matrices)
n_per_layer = 33

# nodes   0..32 are layer0
# nodes  33..65 are layer1
# ...

# layer_vec holds the layer membership of each node
# e.g. layer_vec[5] = 2 means that node 5 resides in layer 2 (the third layer)
layer_vec = [i // n_per_layer for i in range(n_per_layer * num_layers)]
interlayer_edges = [(n_per_layer * layer + v, n_per_layer * layer + v + n_per_layer)
                    for layer in range(num_layers - 1)
                    for v in range(n_per_layer)]


# intralayer edges: we need a list of tuples (i.e. edgelist)
# recode the node indices according to the scheme described above (33..65 is layer1 etc.).
# note that this is unweighted for now (could add weights to the igraph object)
intralayer_edges = []
for i, adj_matrix in enumerate(adj_matrices):
    conn_indices = np.where(adj_matrix)
    x_indices, y_indices = conn_indices
    x_indices += i * n_per_layer
    y_indices += i * n_per_layer
    edges = zip(*(x_indices, y_indices))
    intralayer_edges += edges


G_interlayer = ig.Graph(interlayer_edges)
G_intralayer = ig.Graph(intralayer_edges)


# run leidenalg on a uniform 32x32 grid (1024 samples) of gamma and omega in [0, 2]
gamma_range = (0, 2)
omega_range = (0, 2)
leiden_gammas = np.linspace(*gamma_range, 32)
leiden_omegas = np.linspace(*omega_range, 32)

# TODO: how to set a seed??

# parts = repeated_parallel_leiden_from_gammas_omegas(G_intralayer, G_interlayer, layer_vec, gammas=leiden_gammas, omegas=leiden_omegas)

# TODO: how to set a seed??

# prune to the stable partitions from (gamma=0, omega=0) to (gamma=2, omega=2)
stable_parts = prune_to_multilayer_stable_partitions(G_intralayer, G_interlayer, layer_vec,
                                                     "multiplex", parts,
                                                     *gamma_range, *omega_range)

for p in stable_parts:
    # instead of print(p), we use a more condensed format for the membership vector here
    print(" ".join(str(x) for x in p))

len(stable_parts)

stable_parts[0]

os.getcwd()


# run CHAMP to obtain the dominant partitions along with their regions of optimality
domains = CHAMP_3D(G_intralayer, G_interlayer, layer_vec, parts,
                   gamma_0=gamma_range[0], gamma_f=gamma_range[1],
                   omega_0=omega_range[0], omega_f=omega_range[1])



# append resolution parameter estimates for each dominant partition onto the CHAMP domains
domains_with_estimates = domains_to_gamma_omega_estimates(G_intralayer, G_interlayer, layer_vec,
                                                          domains, model='multiplex')



# plot resolution parameter estimates and domains of optimality
plt.rc('text', usetex=True)
plt.rc('font', family='serif')
plot_2d_domains_with_estimates(domains_with_estimates, xlim=omega_range, ylim=gamma_range)
plt.title(r"CHAMP Domains and ($\omega$, $\gamma$) Estimates", fontsize=16)
plt.xlabel(r"$\omega$", fontsize=20)
plt.ylabel(r"$\gamma$", fontsize=20)
plt.gca().tick_params(axis='both', labelsize=12)
plt.tight_layout()
plt.show()

```



# analyze data 

## descriptives 

```{r}
plot(t12_questionnaireCAMs$total_min_prolific, t12_questionnaireCAMs$total_min_prolific_t2)
cor(t12_questionnaireCAMs$total_min_prolific, t12_questionnaireCAMs$total_min_prolific_t2, use = "pairwise")
```


## correlation plots

### to mean valence

```{r, fig.width=14}
psych::cor.plot(r = cor(t12_questionnaireCAMs[, str_detect(string = colnames(t12_questionnaireCAMs),
                                                   pattern = "^mean_")],
                                                   use = "pairwise.complete.obs"),
                                                   upper = FALSE, xlas = 2, main = "Germany")
```


### to number of concepts


```{r, fig.width=14}
psych::cor.plot(r = cor(t12_questionnaireCAMs[, str_detect(string = colnames(t12_questionnaireCAMs),
                                                   pattern = "^mean_[:alpha:]*$|^num_nodes")],
                                                   use = "pairwise.complete.obs"),
                                                   upper = FALSE, xlas = 2, main = "Germany")
```



### to latent parameters


```{r, fig.width=14}
psych::cor.plot(r = cor(questionnaire_CAM_Germany[, str_detect(string = colnames(questionnaire_CAM_Germany),
                                                   pattern = "^mean_[:alpha:]*$|^density|^transitivity.*macro$|^centr.*macro$|^meanDistance|^assortativity.*macro$")],
                                                   use = "pairwise.complete.obs"),
                                                   upper = FALSE, xlas = 2, main = "Germany")


psych::cor.plot(r = cor(questionnaire_CAM_USA[, str_detect(string = colnames(questionnaire_CAM_USA),
                                                   pattern = "^mean_[:alpha:]*$|^density|^transitivity.*macro$|^centr.*macro$|^meanDistance|^assortativity.*macro$")],
                                                   use = "pairwise.complete.obs"),
                                                   upper = FALSE, xlas = 2, main = "USA")
```


## mean differences

```{r}
# questionnaireCAMs$countryParty <- NA
# questionnaireCAMs$CAM_ID %in% questionnaire_t1$PROLIFIC_PID
# questionnaire_t1$politicalParty

ggbetweenstats(
  data = t12_questionnaireCAMs,
  x = classes_conspiracy,
  y = mean_valence_macro
)


ggbetweenstats(
  data = t12_questionnaireCAMs,
  x = classes_conspiracy,
  y = mean_CMQ
)

ggbetweenstats(
  data = t12_questionnaireCAMs,
  x = classes_conspiracy,
  y = mean_RiskPerception
)

ggbetweenstats(
  data = questionnaireCAMs,
  x = country,
  y = mean_BiosphericValues
)


ggbetweenstats(
  data = questionnaireCAMs,
  x = country,
  y = mean_PolicyItems
)
```

